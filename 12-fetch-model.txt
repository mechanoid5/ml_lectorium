


feature extraction and feature engineering
   – формализация данных, 
     т.е. отображение данных, специфических для предметной области, в точки пространства признаков

feature transformation 
   – трансформация данных для улучшения результатов работы модели (повышения точности)

feature selection 
   – отбор наиболее удачных признаков



Standart Scaling (она же Z-score normalization).

(x - mu / sigma)

в какой-то мере защищает от выбросов

если алгоритм предполагает вычисление расстояний между точками или векторами,
   выбор по умолчанию – StandartScaling


MinMaxScaling
(x - x_min) /(x_max - x_min )




log-трансформация

Параметрические методы обычно требуют как минимум симметричного и унимодального распределения данных, 

 метод ближайших соседей плохо работает, если признаки ненормированы 


Простой пример: предположим, что стоит задача предсказать стоимость квартиры по двум признакам – удаленности от центра и количеству комнат. Количество комнат редко превосходит 5, 
   а расстояние от центра в больших городах легко может измеряться в десятках тысяч метров.


многие модели хорошо работают с нормально распределёнными данными 
если данные описываются распределением похожим на логнормальное
то их можно легко привести нормальному распределению

log(x)



Метод пространственных знаков (spatial sign)
x_ij := x_ij / sum_k=1^P (x_ik)^2

i- примеры, j - признаки, P - количество признаков

проецирует значения предикторов на поверхность многомерной сферы, в результате чего отдельные наблюдения становятся равноудаленными от центра этой сферы

перед применением этой формулы важно выполнить стандартизацию всех предикторов 
(этим будет достигнут примерно одинаковый их вклад в величину квадрата расстояния).
Кроме того, важно понимать, что это преобразование по методу пространственных знаков 
применяется одновременно к группе предикторов. 
Поэтому удаление каких-либо предикторов из анализа после такого преобразования приведет к утрате его смысла.



корреляция признаков и мультиколлинеарность 


Если признаков не очень много, вполне можно сгенерировать все возможные взаимодействия и потом отсеять лишние,
sklearn.preprocessing.PolynomialFeatures








обработка пропусков

заполняем среднем или медианой для вещественных переменных, самое частое для категориальных
наоборот - заменить пропуск на редкое значение
для упорядоченных данных можно брать соседнее значение









отбор признаков


Признаки (feature), используемые для обучения модели, оказывают большое влияние на качество результатов.
Неинформативные или слабо информативные признаки могут существенно понизить эффективность модели.

Уменьшение переобучения. Чем меньше избыточных данных, тем меньше возможностей для модели принимать решения на основе «шума».
Повышение точности. Чем меньше противоречивых данных, тем выше точность.
Сокращение времени обучения. Чем меньше данных, тем быстрее обучается модель.

очевидный кандидат на отстрел – признак, у которого значение неизменно

низковариативные признаки хуже чем высоковариативные.
отсекать признаки, дисперсия которых ниже определенной границы.

from sklearn.feature_selection import SelectKBest, f_classif


Перебор подможеств признаков долго но надёжно
SequentialFeatureSelector


principle components analysis, PCA 


отбор на основе важности признаков







- - - - - - - - - - - - 



12. фичер-инжиниринг

про задачу конструирования признаков

типы признаков

логарифм. преобразование

нормализация (масштабирование)

поиск аномалий

про формирование учебного датасета

методы отбора признаков

выбор модели




- - - - - - - - - - - - 


отбор признаков и моделей, поиск аномалий

оценка качества результатов классификатора

борьба с переобучением
регуляризация L0,L1,L2
скользящий контроль LOO, q-fold CV, t x q - fold CV

перебор подможеств признаков для минимизации ошибки на контроле

добавление признаков по одному с минимизацией ошибки

поочерёдное добавление/удаление 

DFS строим дерево беребора, отрезаем "плохие" ветви

BFS (МГУА) 

случайный поиск с адаптацией (СПА)



Задачи детектирования аномалий не имеют единой формулировки 
и зачастую интерпретируются по-разному в зависимости от характера данных 
и поставленной цели [1, 3, 5]. 

На интуитивном уровне аномалиями называют то, 
что не вписывается в общие правила и законы, 
справедливые для представленных данных. 
Такое определение нуждается в формальном уточнении,
   прежде чем решать задачу математическими методами.



