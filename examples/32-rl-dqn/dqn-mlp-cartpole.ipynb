{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oбучение с подкреплением**\n",
    "\n",
    "Евгений Борисов borisov.e@solarl.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# ENV_NAME = 'BreakoutDeterministic-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1.0 0\n",
      "1: 1.0 0\n",
      "2: 1.0 0\n",
      "3: 1.0 0\n",
      "4: 1.0 0\n",
      "5: 1.0 0\n",
      "6: 1.0 0\n",
      "7: 1.0 0\n",
      "8: 1.0 0\n",
      "9: 1.0 0\n",
      "10: 1.0 0\n",
      "11: 1.0 0\n",
      "12: 1.0 0\n",
      "13: 1.0 0\n",
      "14: 1.0 0\n",
      "15: 1.0 0\n",
      "16: 1.0 0\n",
      "17: 1.0 0\n",
      "18: 1.0 0\n",
      "19: 1.0 0\n",
      "20: 1.0 0\n",
      "21: 1.0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22: 0.0 1\n",
      "23: 0.0 1\n",
      "24: 0.0 1\n",
      "25: 0.0 1\n",
      "26: 0.0 1\n",
      "27: 0.0 1\n",
      "28: 0.0 1\n",
      "29: 0.0 1\n",
      "30: 0.0 1\n",
      "31: 0.0 1\n",
      "32: 0.0 1\n",
      "33: 0.0 1\n",
      "34: 0.0 1\n",
      "35: 0.0 1\n",
      "36: 0.0 1\n",
      "37: 0.0 1\n",
      "38: 0.0 1\n",
      "39: 0.0 1\n",
      "40: 0.0 1\n",
      "41: 0.0 1\n",
      "42: 0.0 1\n",
      "43: 0.0 1\n",
      "44: 0.0 1\n",
      "45: 0.0 1\n",
      "46: 0.0 1\n",
      "47: 0.0 1\n",
      "48: 0.0 1\n",
      "49: 0.0 1\n",
      "50: 0.0 1\n",
      "51: 0.0 1\n",
      "52: 0.0 1\n",
      "53: 0.0 1\n",
      "54: 0.0 1\n",
      "55: 0.0 1\n",
      "56: 0.0 1\n",
      "57: 0.0 1\n",
      "58: 0.0 1\n",
      "59: 0.0 1\n",
      "60: 0.0 1\n",
      "61: 0.0 1\n",
      "62: 0.0 1\n",
      "63: 0.0 1\n",
      "64: 0.0 1\n",
      "65: 0.0 1\n",
      "66: 0.0 1\n",
      "67: 0.0 1\n",
      "68: 0.0 1\n",
      "69: 0.0 1\n",
      "70: 0.0 1\n",
      "71: 0.0 1\n",
      "72: 0.0 1\n",
      "73: 0.0 1\n",
      "74: 0.0 1\n",
      "75: 0.0 1\n",
      "76: 0.0 1\n",
      "77: 0.0 1\n",
      "78: 0.0 1\n",
      "79: 0.0 1\n",
      "80: 0.0 1\n",
      "81: 0.0 1\n",
      "82: 0.0 1\n",
      "83: 0.0 1\n",
      "84: 0.0 1\n",
      "85: 0.0 1\n",
      "86: 0.0 1\n",
      "87: 0.0 1\n",
      "88: 0.0 1\n",
      "89: 0.0 1\n",
      "90: 0.0 1\n",
      "91: 0.0 1\n",
      "92: 0.0 1\n",
      "93: 0.0 1\n",
      "94: 0.0 1\n",
      "95: 0.0 1\n",
      "96: 0.0 1\n",
      "97: 0.0 1\n",
      "98: 0.0 1\n",
      "99: 0.0 1\n",
      "100: 0.0 1\n",
      "101: 0.0 1\n",
      "102: 0.0 1\n",
      "103: 0.0 1\n",
      "104: 0.0 1\n",
      "105: 0.0 1\n",
      "106: 0.0 1\n",
      "107: 0.0 1\n",
      "108: 0.0 1\n",
      "109: 0.0 1\n",
      "110: 0.0 1\n",
      "111: 0.0 1\n",
      "112: 0.0 1\n",
      "113: 0.0 1\n",
      "114: 0.0 1\n",
      "115: 0.0 1\n",
      "116: 0.0 1\n",
      "117: 0.0 1\n",
      "118: 0.0 1\n",
      "119: 0.0 1\n",
      "120: 0.0 1\n",
      "121: 0.0 1\n",
      "122: 0.0 1\n",
      "123: 0.0 1\n",
      "124: 0.0 1\n",
      "125: 0.0 1\n",
      "126: 0.0 1\n",
      "127: 0.0 1\n",
      "128: 0.0 1\n",
      "129: 0.0 1\n",
      "130: 0.0 1\n",
      "131: 0.0 1\n",
      "132: 0.0 1\n",
      "133: 0.0 1\n",
      "134: 0.0 1\n",
      "135: 0.0 1\n",
      "136: 0.0 1\n",
      "137: 0.0 1\n",
      "138: 0.0 1\n",
      "139: 0.0 1\n",
      "140: 0.0 1\n",
      "141: 0.0 1\n",
      "142: 0.0 1\n",
      "143: 0.0 1\n",
      "144: 0.0 1\n",
      "145: 0.0 1\n",
      "146: 0.0 1\n",
      "147: 0.0 1\n",
      "148: 0.0 1\n",
      "149: 0.0 1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.render()\n",
    "for t in range(150):\n",
    "    a = env.action_space.sample()\n",
    "    sn,r,d,_ = env.step(a)\n",
    "    env.render()\n",
    "    print('%d: %.1f %d'%(t,r,d) )\n",
    "    s=sn\n",
    "    #if d: break\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.layers import Dropout\n",
    "# from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(env):\n",
    "    md = Sequential()\n",
    "    md.add(Dense(256, activation='relu',input_shape=env.observation_space.shape))\n",
    "    md.add(Dense(256, activation='relu'))\n",
    "    md.add(Dense(env.action_space.n))\n",
    "    md.compile(loss='mean_squared_error', optimizer='SGD')\n",
    "    \n",
    "    #md.add(Dense(env.action_space.n, activation='softmax'))\n",
    "    #md.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4))\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(env)\n",
    "model_target = create_model(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_DEEP = 2000 # глубина истории переходов\n",
    "BATCH_SIZE = 32 # размер батча для обучения модели\n",
    "\n",
    "GAMMA = 0.85 # коэффициент для расчёта оценки reward\n",
    "TAU = 0.125 # определяет различие весов основной и таргет моделей\n",
    "\n",
    "# вероятность использования случайного действия при обучении\n",
    "EPSILON_MAX = 1.0 # в начальном состоянии используем случайный выбор действия\n",
    "EPSILON_MIN = 0.05 # когда модель начала обучаться позволяем ей оценивать действия\n",
    "EPSILON_DECAY = 0.995 # шаг изменения вероятности\n",
    "\n",
    "MAX_EPOCH = 100 # максимальное количество эпох обучения\n",
    "MAX_STEP = 256  # максимальное количество шагов в эпохе (длительность одной игры)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_state(s): return s[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбор действия\n",
    "def get_action(model,state,eps=EPSILON_MAX): # с вероятностью eps выбираем случайное действие\n",
    "    if np.random.rand()<eps: return env.action_space.sample(), 1\n",
    "    return np.argmax( model.predict( state )[0] ), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(model, model_target, history, batch_size=BATCH_SIZE,gamma=GAMMA):\n",
    "\n",
    "    if len(history) < batch_size: return\n",
    "    \n",
    "    samples = [ history[i] for i in np.random.permutation(len(history))[:batch_size] ] \n",
    "\n",
    "    for s in samples:\n",
    "        target = model_target.predict( s['state0'] )\n",
    "        \n",
    "        target[0][ s['act'] ] = s['reward'] \n",
    "        \n",
    "        if s['isdone']:\n",
    "            target[0][ s['act'] ] = s['reward']\n",
    "        else:\n",
    "            target[0][ s['act'] ] = s['reward'] + gamma*max(model_target.predict(s['state1'])[0])\n",
    "            \n",
    "        # model.fit( s['state0'], target, epochs=1, verbose=0)\n",
    "        model.train_on_batch(s['state0'], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_train(model, model_target,tau=TAU):\n",
    "    weights = model.get_weights()\n",
    "    target_weights = model_target.get_weights()\n",
    "    for i in range(len(target_weights)):\n",
    "        target_weights[i] = weights[i] * tau + target_weights[i] * (1. - tau)\n",
    "    model_target.set_weights(target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 : 23 steps\n",
      "2/100 : 14 steps\n",
      "3/100 : 45 steps\n",
      "4/100 : 32 steps\n",
      "5/100 : 46 steps\n",
      "6/100 : 73 steps\n",
      "7/100 : 129 steps\n",
      "8/100 : 96 steps\n",
      "9/100 : 98 steps\n",
      "10/100 : 22 steps\n",
      "11/100 : 187 steps\n",
      "12/100 : 136 steps\n",
      "13/100 : 226 steps\n",
      "STEP BOUND!\n",
      "CPU times: user 52min 8s, sys: 53.7 s, total: 53min 2s\n",
      "Wall time: 51min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = []\n",
    "\n",
    "epsilon = EPSILON_MAX # вероятность использования случайного действия при обучении\n",
    "\n",
    "for e in range(MAX_EPOCH):\n",
    "    # инициализируем среду\n",
    "    state0 = env.reset() \n",
    "    state0 = reshape_state(state0)\n",
    "    for st in range(MAX_STEP):\n",
    "        \n",
    "        # выбираем действие\n",
    "        act, is_act_rand = get_action(model,state0,epsilon)\n",
    "        # применяем действие\n",
    "        state1, reward, done, _ = env.step(act)\n",
    "        state1 = reshape_state(state1)\n",
    "    \n",
    "        # сохраняем историю\n",
    "        history.append({'state0':state0,'state1':state1,'act':act,'reward':reward,'isdone':done})\n",
    "        history = history[-HISTORY_DEEP:] # ограничиваем историю\n",
    "\n",
    "        # коррекция основной модели\n",
    "        replay(model, model_target, history)\n",
    "        \n",
    "        # коррекция таргет модели\n",
    "        target_train(model, model_target)\n",
    "                \n",
    "        if done: break\n",
    "        state0 = state1\n",
    "        epsilon = max(EPSILON_MIN, epsilon*EPSILON_DECAY)\n",
    "                \n",
    "    print('%d/%d : %d steps'%(e+1,MAX_EPOCH,st+1))  \n",
    "    \n",
    "    if st>200: \n",
    "        print('STEP BOUND!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(history)\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: CartPole-225.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('CartPole-%d.model'%(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# model = load_model('CartPole-12.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()[np.newaxis,:]\n",
    "env.render()\n",
    "#while True:\n",
    "for t in range(350):    \n",
    "    a = np.argmax( model.predict(s)[0] )   \n",
    "    s,r,d,_ = env.step(a)\n",
    "    s=s[np.newaxis,:]\n",
    "    env.render()\n",
    "    if d: \n",
    "        print(t) \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
