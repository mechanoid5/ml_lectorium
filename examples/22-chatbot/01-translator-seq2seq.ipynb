{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**автоматический переводчик на основе рекуррентных нейросетей seq2seq**\n",
    "\n",
    "Евгений Борисов borisov.e@solarl.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(d): return \"{:,.0f}\".format(d).replace(\",\", \" \")\n",
    "def ppr(d): print('записей:', pp(len(d)) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 336 667\n"
     ]
    }
   ],
   "source": [
    "# http://www.manythings.org/anki/\n",
    "    \n",
    "# список фраз на английском с переводом на русский\n",
    "with open('../data/text/rus-eng/rus.txt', 'rt', encoding='utf-8') as f:\n",
    "    lines = f.read().lower().split('\\n')\n",
    "\n",
    "ppr(lines)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 10 000\n"
     ]
    }
   ],
   "source": [
    "# фразы упорядоченны по длине, выберем среднюю длину\n",
    "lines = [ lines[i] for i in range(100000,110000) ]\n",
    "ppr(lines)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбираем строки на русские и английские с сохранением порядка\n",
    "\n",
    "# определим специальные символы - начало и конец фразы\n",
    "GO='\\t' # символ <старт>\n",
    "EOS='\\n' # символ <стоп>\n",
    "\n",
    "input_texts  = [ s.split('\\t')[0] for s in lines if s ] \n",
    "target_texts = [ GO + ' ' + s.split('\\t')[1]+ ' ' + EOS for s in lines if s ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# входной и выходной алфавиты\n",
    "input_characters  = sorted(set(' '.join(input_texts)))\n",
    "target_characters = sorted(set(' '.join(target_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размер входного алфавита\n",
    "num_encoder_tokens = len(input_characters) \n",
    "# максимальная длина входной фразы в символах \n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "\n",
    "# размер выходного алфавита\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# максимальная длина выходной фразы в символах \n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нумеруем символы в алфавите\n",
    "input_token_index = { char:i for i, char in enumerate(input_characters) }\n",
    "target_token_index = { char:i  for i, char in enumerate(target_characters) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## кодируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим статистическую модель порождения текста \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\n",
    "#     Holger Schwenk, Yoshua Bengio\n",
    "# Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "# 3 Sep 2014\n",
    "# https://arxiv.org/abs/1406.1078\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# statistical machine translation system (SMT)\n",
    "\n",
    "# the goal of the system (decoder,specifically) is to \n",
    "# find a translation f given a source sentence e, which maximizes\n",
    "\n",
    "# p(f|e) ∝ p(e|f)p(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для каждого примера\n",
    "#   строим таблицу индикаторов  {0,1}\n",
    "#     [ номер слова в последовательности, номер символа в алфавите ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные энкодера \n",
    "\n",
    "# входная последовательность генерирует выход по схеме many2one \n",
    "#   выход энкодера выкидываем\n",
    "#    используем только его конечное состояние\n",
    "#      первым входом декодера есть служебное слово <пуск>\n",
    "encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for s, input_text in enumerate(input_texts):\n",
    "    for w, c in enumerate(input_text):\n",
    "        encoder_input_data[s, w, input_token_index[c]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные декодера (для целевой последовательности), \n",
    "\n",
    "# память декодера инициализируеться конечным состянием памяти энкодера \n",
    "#   и на вход подаём служебное слово <пуск>    \n",
    "#    далее рекурсивно - очередной выход декодера подаёться на вход и генерирует следующий выход\n",
    "\n",
    "# вход декодера\n",
    "decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "# выход декодера (вход смещённый на один шаг)\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, target_text in enumerate(target_texts):\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 512  # размер сети\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 47)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 67)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 1146880     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  1187840     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 67)     34371       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,369,091\n",
      "Trainable params: 2,369,091\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "7900/8000 [============================>.] - ETA: 1s - loss: 1.1880"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=100,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = { i:char for char,i in input_token_index.items() }\n",
    "reverse_target_char_index = { i:char for char,i in target_token_index.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # генерируем состояние энкодера\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # вход декодера - последовательность из одного слова GO\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index[GO]] = 1.\n",
    "\n",
    "    # выходную последовательность\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    for i in range(max_decoder_seq_length): \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # декодируем символ\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # если очередной символ это EOS\n",
    "        if(sampled_char==EOS): break # то завершаем работу\n",
    "\n",
    "        # обновляем входную последовательность\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # обновляем состояние сети\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = np.random.permutation(len(encoder_input_data))[:100]\n",
    "for seq_index in ii:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print( input_texts[seq_index],' -> ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tensorflow:', tf.__version__)\n",
    "print('keras:', keras.__version__)\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print('GPU devices:\\n  ',\n",
    "        [ [x.name, x.physical_device_desc] \n",
    "          for x in device_lib.list_local_devices() \n",
    "          if x.device_type == 'GPU' ]\n",
    "    )\n",
    "    print('default GPU device:', tf.test.gpu_device_name() )\n",
    "\n",
    "else:\n",
    "    print('no GPU device found')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# #Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "# This script demonstrates how to implement a basic character-level\n",
    "# sequence-to-sequence model. We apply it to translating\n",
    "# short English sentences into short French sentences,\n",
    "# character-by-character. Note that it is fairly unusual to\n",
    "# do character-level machine translation, as word-level\n",
    "# models are more common in this domain.\n",
    "\n",
    "# **Summary of the algorithm**\n",
    "\n",
    "# - We start with input sequences from a domain (e.g. English sentences)\n",
    "#     and corresponding target sequences from another domain\n",
    "#     (e.g. French sentences).\n",
    "# - An encoder LSTM turns input sequences to 2 state vectors\n",
    "#     (we keep the last LSTM state and discard the outputs).\n",
    "# - A decoder LSTM is trained to turn the target sequences into\n",
    "#     the same sequence but offset by one timestep in the future,\n",
    "#     a training process called \"teacher forcing\" in this context.\n",
    "#     It uses as initial state the state vectors from the encoder.\n",
    "#     Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "#     given `targets[...t]`, conditioned on the input sequence.\n",
    "# - In inference mode, when we want to decode unknown input sequences, we:\n",
    "#     - Encode the input sequence into state vectors\n",
    "#     - Start with a target sequence of size 1\n",
    "#         (just the start-of-sequence character)\n",
    "#     - Feed the state vectors and 1-char target sequence\n",
    "#         to the decoder to produce predictions for the next character\n",
    "#     - Sample the next character using these predictions\n",
    "#         (we simply use argmax).\n",
    "#     - Append the sampled character to the target sequence\n",
    "#     - Repeat until we generate the end-of-sequence character or we\n",
    "#         hit the character limit.\n",
    "\n",
    "# **Data download**\n",
    "\n",
    "# [English to French sentence pairs.\n",
    "# ](http://www.manythings.org/anki/fra-eng.zip)\n",
    "\n",
    "# [Lots of neat sentence pairs datasets.\n",
    "# ](http://www.manythings.org/anki/)\n",
    "\n",
    "# **References**\n",
    "\n",
    "# - [Sequence to Sequence Learning with Neural Networks\n",
    "#    ](https://arxiv.org/abs/1409.3215)\n",
    "# - [Learning Phrase Representations using\n",
    "#     RNN Encoder-Decoder for Statistical Machine Translation\n",
    "#     ](https://arxiv.org/abs/1406.1078)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_characters\n",
    "# input_characters\n",
    "# input_texts\n",
    "# output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 10000  # Number of samples to train on.\n",
    "\n",
    "# # Vectorize the data.\n",
    "# input_texts = []\n",
    "# target_texts = []\n",
    "# input_characters = set()\n",
    "# target_characters = set()\n",
    "\n",
    "# with open('../data/text/rus-eng/rus.txt', 'rt', encoding='utf-8') as f:\n",
    "#     lines = f.read().split('\\n')\n",
    "    \n",
    "# for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "#     input_text, target_text = line.split('\\t')\n",
    "#     # We use \"tab\" as the \"start sequence\" character\n",
    "#     # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     target_text = '\\t' + target_text + '\\n'\n",
    "#     input_texts.append(input_text)\n",
    "#     target_texts.append(target_text)\n",
    "#     for char in input_text:\n",
    "#         if char not in input_characters:\n",
    "#             input_characters.add(char)\n",
    "#     for char in target_text:\n",
    "#         if char not in target_characters:\n",
    "#             target_characters.add(char)\n",
    "\n",
    "# input_characters = sorted(list(input_characters))\n",
    "# target_characters = sorted(list(target_characters))\n",
    "# num_encoder_tokens = len(input_characters)\n",
    "# num_decoder_tokens = len(target_characters)\n",
    "# max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "#     for t, char in enumerate(input_text):\n",
    "#         encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "#     for t, char in enumerate(target_text):\n",
    "#         # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "#         decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "#         if t > 0:\n",
    "#             # decoder_target_data will be ahead by one timestep\n",
    "#             # and will not include the start character.\n",
    "#             decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next: inference mode (sampling).\n",
    "# # Here's the drill:\n",
    "# # 1) encode input and retrieve initial decoder state\n",
    "# # 2) run one step of decoder with this initial state\n",
    "# # and a \"start of sequence\" token as target.\n",
    "# # Output will be the next target token\n",
    "# # 3) Repeat with the current target token and current states\n",
    "\n",
    "# # Define sampling models\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq_index in range(100):\n",
    "#     # Take one sequence (part of the training set)\n",
    "#     # for trying out decoding.\n",
    "#     input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "#     print('-')\n",
    "#     print('Input sentence:', input_texts[seq_index])\n",
    "#     print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
