{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(d): return \"{:,.0f}\".format(d).replace(\",\", \" \")\n",
    "def ppr(d): print('записей:', pp(len(d)) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 336 667\n"
     ]
    }
   ],
   "source": [
    "with open('../data/text/rus-eng/rus.txt', 'rt', encoding='utf-8') as f:\n",
    "    lines = f.read().lower().split('\\n')\n",
    "\n",
    "ppr(lines)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 10 000\n"
     ]
    }
   ],
   "source": [
    "lines = [ lines[i] for i in range(100000,110000) ]\n",
    "ppr(lines)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO='\\t' # символ <старт>\n",
    "EOS='\\n' # символ <стоп>\n",
    "\n",
    "input_texts  = [ s.split('\\t')[0] for s in lines if s ] \n",
    "target_texts = [ GO + ' ' + s.split('\\t')[1]+ ' ' + EOS for s in lines if s ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters  = sorted(set(' '.join(input_texts)))\n",
    "target_characters = sorted(set(' '.join(target_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of samples:', len(input_texts))\n",
    "# print('Number of unique input tokens:', num_encoder_tokens)\n",
    "# print('Number of unique output tokens:', num_decoder_tokens)\n",
    "# print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = { char:i for i, char in enumerate(input_characters) }\n",
    "target_token_index = { char:i  for i, char in enumerate(target_characters) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "для каждого примера\n",
    "  строим таблицу индикаторов  {0,1}\n",
    "    [ номер слова в последовательности, номер символа в алфавите ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные энкодера \n",
    "\n",
    "# входная последовательность генерирует выход по схеме many2one \n",
    "#   выход энкодера выкидываем\n",
    "#    используем только его конечное состояние\n",
    "#      первым входом декодера есть служебное слово <пуск>\n",
    "encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for s, input_text in enumerate(input_texts):\n",
    "    for w, c in enumerate(input_text):\n",
    "        encoder_input_data[s, w, input_token_index[c]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные декодера (для целевой последовательности), \n",
    "\n",
    "# память декодера инициализируеться конечным состянием памяти энкодера \n",
    "#   и на вход подаём служебное слово <пуск>    \n",
    "#    далее рекурсивно - очередной выход декодера подаёться на вход и генерирует следующий выход\n",
    "\n",
    "# вход декодера\n",
    "decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "# выход декодера (вход смещённый на один шаг)\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, target_text in enumerate(target_texts):\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 512  # размер сети\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 47)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 67)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 1146880     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  1187840     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 67)     34371       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,369,091\n",
      "Trainable params: 2,369,091\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 101s 13ms/sample - loss: 1.1819 - val_loss: 1.1133\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 98s 12ms/sample - loss: 0.9537 - val_loss: 0.9327\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 101s 13ms/sample - loss: 0.8389 - val_loss: 0.8801\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 101s 13ms/sample - loss: 0.7722 - val_loss: 0.8410\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 101s 13ms/sample - loss: 0.7179 - val_loss: 0.8047\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 99s 12ms/sample - loss: 0.6731 - val_loss: 0.7783\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 102s 13ms/sample - loss: 0.6397 - val_loss: 0.7553\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 100s 13ms/sample - loss: 0.6034 - val_loss: 0.7346\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 99s 12ms/sample - loss: 0.5752 - val_loss: 0.7174\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 98s 12ms/sample - loss: 0.5479 - val_loss: 0.7050\n",
      "CPU times: user 8min 57s, sys: 6min 53s, total: 15min 50s\n",
      "Wall time: 16min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = { i:char for char,i in input_token_index.items() }\n",
    "reverse_target_char_index = { i:char for char,i in target_token_index.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # генерируем состояние энкодера\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # вход декодера - последовательность из одного слова GO\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index[GO]] = 1.\n",
    "\n",
    "    # выходную последовательность\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    for i in range(max_decoder_seq_length): \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # декодируем символ\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # если очередной символ это EOS\n",
    "        if(sampled_char==EOS): break # то завершаем работу\n",
    "\n",
    "        # обновляем входную последовательность\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # обновляем состояние сети\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the classroom is clean.  ->   они вас не проворовали. \n",
      "\n",
      "tom is obviously angry.  ->   том не вадит правоть. \n",
      "\n",
      "tom said he'd call you.  ->   том не вадит правоть. \n",
      "\n",
      "read the article again.  ->   они вас не проворовали. \n",
      "\n",
      "i've got lots of ideas.  ->   я не вас на праводали. \n",
      "\n",
      "i'll phone again later.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "do you know how to fish?  ->   давай постором поможеть мого. \n",
      "\n",
      "they launched a rocket.  ->   они вас не проворовали. \n",
      "\n",
      "tom will paint his car.  ->   том не вадит правоть. \n",
      "\n",
      "what is the book about?  ->   что ты пришло подоворить? \n",
      "\n",
      "tom didn't like school.  ->   том не вадит правоть. \n",
      "\n",
      "tom screamed in terror.  ->   том не вадит правоть. \n",
      "\n",
      "they all looked at you.  ->   они вас не проворовали. \n",
      "\n",
      "you don't need to wait.  ->   ты не надо бы проворовать. \n",
      "\n",
      "no one likes this idea.  ->   это было не поможете. \n",
      "\n",
      "see you in three weeks.  ->   они вас не проворовали. \n",
      "\n",
      "tell me why you did it.  ->   скажи мне, что вы были тем. \n",
      "\n",
      "i'm not your boyfriend.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "don't tell tom anything.  ->   это было не поможете. \n",
      "\n",
      "europe is not a country.  ->   они вас не проворовали. \n",
      "\n",
      "he likes playing soccer.  ->   они вас не проворовали. \n",
      "\n",
      "why don't you call tom?  ->   почему ты не нажно помочь? \n",
      "\n",
      "i'm completely serious.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "show me where it hurts.  ->   это было не поможете. \n",
      "\n",
      "did everybody hear that?  ->   это было не поможете. \n",
      "\n",
      "we're working together.  ->   мы все дали мэри помощь. \n",
      "\n",
      "washington was worried.  ->   почему ты не нажно помочь? \n",
      "\n",
      "tom always exaggerates.  ->   том не вадит правоть. \n",
      "\n",
      "can you count in french?  ->   они вас не проворовали. \n",
      "\n",
      "tom was in a good mood.  ->   том не вадит правоть. \n",
      "\n",
      "we need to do the same.  ->   мы все дали мэри помощь. \n",
      "\n",
      "tom will agree with me.  ->   том не вадит правоть. \n",
      "\n",
      "tom kept his eyes shut.  ->   том не вадит правоть. \n",
      "\n",
      "don't tell me, tell tom.  ->   это было не поможете. \n",
      "\n",
      "do you know mr. jackson?  ->   давай постором поможеть мого. \n",
      "\n",
      "he caught a large trout.  ->   они вас не проворовали. \n",
      "\n",
      "you must work together.  ->   ты не надо бы проворовать. \n",
      "\n",
      "tom is mary's neighbor.  ->   том не вадит правоть. \n",
      "\n",
      "do you want to see this?  ->   давай постором поможеть мого. \n",
      "\n",
      "tom wasn't too careful.  ->   том не вадит правоть. \n",
      "\n",
      "i'll talk to tom later.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "what did tom tell mary?  ->   что ты пришло подоворить? \n",
      "\n",
      "i'll never forgive tom.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "tom often loses things.  ->   том не вадит правоть. \n",
      "\n",
      "tom removed his helmet.  ->   том не вадит правоть. \n",
      "\n",
      "i'm ashamed of my past.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "tom never eats spinach.  ->   том не вадит правоть. \n",
      "\n",
      "we'll be here all week.  ->   мы сказал, что вы можете помочь. \n",
      "\n",
      "choose the one you like.  ->   это было не поможете. \n",
      "\n",
      "tom should've said yes.  ->   том не вадит правоть. \n",
      "\n",
      "the barn door was open.  ->   они вас не проворовали. \n",
      "\n",
      "we had good intentions.  ->   мы все дали мэри помощь. \n",
      "\n",
      "you caught three birds.  ->   ты не надо бы проворовать. \n",
      "\n",
      "that was the hard part.  ->   я не вад правотовали. \n",
      "\n",
      "they skinned him alive.  ->   они вас не проворовали. \n",
      "\n",
      "what's your first name?  ->   что ты пришло подоворить? \n",
      "\n",
      "you're not very polite.  ->   ты не надо бы проворовать. \n",
      "\n",
      "i'm at tom's apartment.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "we're both very hungry.  ->   мы все дали мэри помощь. \n",
      "\n",
      "the container is empty.  ->   они вас не проворовали. \n",
      "\n",
      "what's tom writing now?  ->   что ты пришло подоворить? \n",
      "\n",
      "i'll put it in the box.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "my father will kill me.  ->   моя было было было праводом. \n",
      "\n",
      "did you want to join us?  ->   это было не поможете. \n",
      "\n",
      "i'm writing a book now.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "tom loves architecture.  ->   том не вадит правоть. \n",
      "\n",
      "why is truth important?  ->   почему ты не нажно помочь? \n",
      "\n",
      "the ground is too soft.  ->   они вас не проворовали. \n",
      "\n",
      "tom also speaks french.  ->   том не вадит правоть. \n",
      "\n",
      "it was really horrible.  ->   я не вас на праводали. \n",
      "\n",
      "well, you may be right.  ->   мы сказал, что вы можете помочь. \n",
      "\n",
      "we know why you did it.  ->   мы все дали мэри помощь. \n",
      "\n",
      "they know where we are.  ->   они вас не проворовали. \n",
      "\n",
      "i'm going to work here.  ->   я не ваш на очень хорошо. \n",
      "\n",
      "what do you want to do?  ->   что ты пришло подоворить? \n",
      "\n",
      "it's far too dangerous.  ->   я не вад правотовали. \n",
      "\n",
      "let me show you around.  ->   скажи мне, что вы были тем. \n",
      "\n",
      "where is the newspaper?  ->   почему ты не нажно помочь? \n",
      "\n",
      "incentives always help.  ->   это было не поможете. \n",
      "\n",
      "they don't despise you.  ->   они вас не проворовали. \n",
      "\n",
      "do you love your mother?  ->   давай постором поможеть мого. \n",
      "\n",
      "we're learning chinese.  ->   мы все дали мэри помощь. \n",
      "\n",
      "that makes me so angry.  ->   я не вад так пороворовалься. \n",
      "\n",
      "why don't you go there?  ->   почему ты не нажно помочь? \n",
      "\n",
      "eat up all your spinach!  ->   они вас не проворовали. \n",
      "\n",
      "he had no place to live.  ->   они вас не проворовали. \n",
      "\n",
      "let me take your coats.  ->   скажи мне, что вы были тем. \n",
      "\n",
      "i've got to make lunch.  ->   я не вас на праводали. \n",
      "\n",
      "there were two bridges.  ->   они вас не проворовали. \n",
      "\n",
      "tell me you're kidding.  ->   скажи мне, что вы были тем. \n",
      "\n",
      "you can go if you want.  ->   ты не надо бы проворовать. \n",
      "\n",
      "when is your next show?  ->   почему ты не нажно помочь? \n",
      "\n",
      "we must stick together.  ->   мы сказали, что вы были том. \n",
      "\n",
      "tom and mary are happy.  ->   том не вадит правоть. \n",
      "\n",
      "he returned from canada.  ->   они вас не проворовали. \n",
      "\n",
      "why did you turn me in?  ->   почему ты не нажно помочь? \n",
      "\n",
      "why aren't you at home?  ->   почему ты не нажно помочь? \n",
      "\n",
      "tom said you were dead.  ->   том не вадит правоть. \n",
      "\n",
      "don't be so sentimental.  ->   это было не поможете. \n",
      "\n",
      "she knows herself well.  ->   они вас не проворовали. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ii = np.random.permutation(len(encoder_input_data))[:100]\n",
    "for seq_index in ii:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print( input_texts[seq_index],' -> ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4lOXSwOHfGDqhg9IFFJVigBgRBaUjNlBEBcV2RD70KCo2QPTYxa4odgWPIMixoseuIHpUeu9IkQjSFASkJcz3x2ySTUhCQrJ5N8nc17WXW959d3YjO/u0eURVcc455wCOCDoA55xz0cOTgnPOuVSeFJxzzqXypOCccy6VJwXnnHOpPCk455xL5UnB5SsRiRGRnSJSPz+PDZKIHCsi+T53W0S6iMiasNvLROT0nBx7GK/1mogMO9znZ3PeB0VkTH6f1wWnRNABuGCJyM6wm+WAvUBy6Pb/qeq43JxPVZOB2Pw+tjhQ1ePz4zwi0h/op6odws7dPz/O7Yo+TwrFnKqmfimHfon2V9WvszpeREqoalJBxOacK3jefeSyFeoeeEdExovIDqCfiJwqIj+LyDYR2SAiI0WkZOj4EiKiItIgdHts6PHPRGSHiPwkIg1ze2zo8bNEZLmIbBeR50TkfyJyVRZx5yTG/xORlSLyp4iMDHtujIg8LSJbReQXoHs2n89wEZmQ4b5RIvJU6Hp/EVkSej+/hH7FZ3WuRBHpELpeTkTeCsW2CDgpk9ddFTrvIhHpEbr/ROB54PRQ19yWsM/23rDnDwy9960i8qGI1MrJZ3MoInJ+KJ5tIvKtiBwf9tgwEVkvIn+JyNKw99pGRGaH7t8oIo/n9PVcBKiqX/yCqgKsAbpkuO9BYB9wHvYjoixwMnAK1tJsBCwHbggdXwJQoEHo9lhgC5AAlATeAcYexrFHAjuAnqHHBgP7gauyeC85ifEjoBLQAPgj5b0DNwCLgLpANWCq/VPJ9HUaATuB8mHn3gQkhG6fFzpGgE7AbiAu9FgXYE3YuRKBDqHrTwBTgCrA0cDiDMdeDNQK/U0uDcVwVOix/sCUDHGOBe4NXe8WirElUAZ4Afg2J59NJu//QWBM6HqTUBydQn+jYaHPvSTQDFgL1Awd2xBoFLo+A+gbul4BOCXofwvF+eItBZcTP6jqx6p6QFV3q+oMVZ2mqkmqugp4BWifzfPfVdWZqrofGId9GeX22HOBuar6Ueixp7EEkqkcxviIqm5X1TXYF3DKa10MPK2qiaq6FRiRzeusAhZiyQqgK7BNVWeGHv9YVVep+Rb4Bsh0MDmDi4EHVfVPVV2L/foPf92Jqroh9Dd5G0voCTk4L8BlwGuqOldV9wBDgPYiUjfsmKw+m+z0ASap6rehv9EIoCKWnJOwBNQs1AW5OvTZgSX3xiJSTVV3qOq0HL4PFwGeFFxOrAu/ISIniMh/ReR3EfkLuB+ons3zfw+7/jfZDy5ndWzt8DhUVbFf1pnKYYw5ei3sF2523gb6hq5fiiWzlDjOFZFpIvKHiGzDfqVn91mlqJVdDCJylYjMC3XTbANOyOF5wd5f6vlU9S/gT6BO2DG5+Ztldd4D2N+ojqouA27F/g6bQt2RNUOHXg00BZaJyHQROTuH78NFgCcFlxMZp2O+jP06PlZVKwL3YN0jkbQB684BQESE9F9iGeUlxg1AvbDbh5oy+w7QJfRLuyeWJBCRssC7wCNY105l4MscxvF7VjGISCPgReA6oFrovEvDznuo6bPrsS6plPNVwLqpfstBXLk57xHY3+w3AFUdq6ptsa6jGOxzQVWXqWofrIvwSeA9ESmTx1jcYfKk4A5HBWA7sEtEmgD/VwCv+QkQLyLniUgJ4CagRoRinAjcLCJ1RKQacGd2B6vqRuAHYDSwTFVXhB4qDZQCNgPJInIu0DkXMQwTkcpi6zhuCHssFvvi34zlx/5YSyHFRqBuysB6JsYD14hInIiUxr6cv1fVLFteuYi5h4h0CL327dg40DQRaSIiHUOvtzt0ScbewOUiUj3Ustgeem8H8hiLO0yeFNzhuBW4EvsH/zL2SzmiQl+8lwBPAVuBY4A52LqK/I7xRazvfwE2CPpuDp7zNjZw/HZYzNuAW4APsMHa3lhyy4l/YS2WNcBnwL/DzjsfGAlMDx1zAhDeD/8VsALYKCLh3UApz/8c68b5IPT8+tg4Q56o6iLsM38RS1jdgR6h8YXSwGPYONDvWMtkeOipZwNLxGa3PQFcoqr78hqPOzxiXbPOFS4iEoN1V/RW1e+Djse5osJbCq7QEJHuIlIp1AVxNzajZXrAYTlXpHhScIVJO2AV1gXRHThfVbPqPnLOHQbvPnLOOZfKWwrOOedSFbqCeNWrV9cGDRoEHYZzzhUqs2bN2qKq2U3jBiKYFETkDaw0wSZVbZ7J45eRNv97J3Cdqs471HkbNGjAzJkz8zVW55wr6kTkUCvzgch2H40hm+qSwGqgvarGAQ9gtWmcc84FKGItBVWdKqGSyFk8/mPYzZ8JK2HgnHMuGNEy0HwNtmrTOedcgAIfaBaRjlhSaJfNMQOAAQD160f1dr7OFTn79+8nMTGRPXv2BB2Ky4EyZcpQt25dSpbMqvRV9gJNCiISB7wGnBWqW58pVX2F0JhDQkKCL6xwrgAlJiZSoUIFGjRogBWnddFKVdm6dSuJiYk0bNjw0E/IRGDdR6HKj+8Dl6vq8qDicM5lb8+ePVSrVs0TQiEgIlSrVi1PrbpITkkdD3QAqotIIlb1sSSAqr6E1bevBrwQ+p8tSVVzunOUc64AeUIoPPL6t4rk7KO+h3i8P7aXbIFYsQJGjYLHH4fD7GpzzrkiL1pmH0Xc8uXw7LMwfnzQkTjncmPr1q20bNmSli1bUrNmTerUqZN6e9++nG27cPXVV7Ns2bJsjxk1ahTjxo3L9picateuHXPnzs2XcxW0wGcfFZSzz4a4OHjkEejXD44oNunQucKtWrVqqV+w9957L7Gxsdx2223pjlFVVJUjsviHPXr06EO+zj//+c+8B1sEFJuvRhEYOhSWLoUPPww6GudcXq1cuZLmzZszcOBA4uPj2bBhAwMGDCAhIYFmzZpx//33px6b8ss9KSmJypUrM2TIEFq0aMGpp57Kpk2bABg+fDjPPPNM6vFDhgyhdevWHH/88fz4o6213bVrFxdeeCEtWrSgb9++JCQkHLJFMHbsWE488USaN2/OsGHDAEhKSuLyyy9PvX/kyJEAPP300zRt2pQWLVrQr1+/fP/McqLYtBQALroI7r7bWgsXXGCJwjmXczffDPndK9KyJYS+i3Nt8eLFjB49mpdeegmAESNGULVqVZKSkujYsSO9e/emadOm6Z6zfft22rdvz4gRIxg8eDBvvPEGQ4YMOejcqsr06dOZNGkS999/P59//jnPPfccNWvW5L333mPevHnEx8dnG19iYiLDhw9n5syZVKpUiS5duvDJJ59Qo0YNtmzZwoIFCwDYtm0bAI899hhr166lVKlSqfcVtGLTUgCIiYE774SZM+Hrr4OOxjmXV8cccwwnn3xy6u3x48cTHx9PfHw8S5YsYfHixQc9p2zZspx11lkAnHTSSaxZsybTc/fq1eugY3744Qf69OkDQIsWLWjWrFm28U2bNo1OnTpRvXp1SpYsyaWXXsrUqVM59thjWbZsGTfddBNffPEFlSpVAqBZs2b069ePcePGHfbis7wqVi0FgMsvh3/9Cx5+GLp2DToa5wqXw/1FHynly5dPvb5ixQqeffZZpk+fTuXKlenXr1+m8/VLlSqVej0mJoakpKRMz126dOmDjsntpmRZHV+tWjXmz5/PZ599xsiRI3nvvfd45ZVX+OKLL/juu+/46KOPePDBB1m4cCExMTG5es28KlYtBYDSpeG222DKFPjpp6Cjcc7ll7/++osKFSpQsWJFNmzYwBdffJHvr9GuXTsmTpwIwIIFCzJtiYRr06YNkydPZuvWrSQlJTFhwgTat2/P5s2bUVUuuugi7rvvPmbPnk1ycjKJiYl06tSJxx9/nM2bN/P333/n+3s4lGLXUgC49lp46CEbW5g0KehonHP5IT4+nqZNm9K8eXMaNWpE27Zt8/01brzxRq644gri4uKIj4+nefPmqV0/malbty73338/HTp0QFU577zzOOecc5g9ezbXXHMNqoqI8Oijj5KUlMSll17Kjh07OHDgAHfeeScVKlTI9/dwKIVuj+aEhATNj012HngA7rkH5s2zqarOucwtWbKEJk2aBB1GVEhKSiIpKYkyZcqwYsUKunXrxooVKyhRIrp+X2f2NxORWTmpGlHsuo9S3HADxMbCiBFBR+KcKyx27txJ27ZtadGiBRdeeCEvv/xy1CWEvCpa7yYXqlSB666DJ5+0VsMxxwQdkXMu2lWuXJlZs2YFHUZEFduWAsAtt1gdpMceCzoS55yLDsU6KdSqBVdfDWPGwPr1QUfjnHPBK9ZJAeD22yE5GZ56KuhInHMueMU+KTRqBH37wksvwdYs935zzrniodgnBYAhQ2DXLnjuuaAjcc5l1KFDh4MWoj3zzDNcf/312T4vNjYWgPXr19O7d+8sz32oKe7PPPNMukVkZ599dr7UJbr33nt54okn8nye/OZJAWjWDHr2hJEjYceOoKNxzoXr27cvEyZMSHffhAkT6Ns32328UtWuXZt33333sF8/Y1L49NNPqVy58mGfL9p5UggZOhT+/BNeeSXoSJxz4Xr37s0nn3zC3r17AVizZg3r16+nXbt27Ny5k86dOxMfH8+JJ57IRx99dNDz16xZQ/PmzQHYvXs3ffr0IS4ujksuuYTdu3enHnfdddellt3+17/+BcDIkSNZv349HTt2pGPHjgA0aNCALVu2APDUU0/RvHlzmjdvnlp2e82aNTRp0oRrr72WZs2a0a1bt3Svk5m5c+fSpk0b4uLiuOCCC/jzzz9TX79p06bExcWlFuL77rvvUjcZatWqFTvy+ZdssV2nkNEpp0DnzrZu4Z//hDJlgo7IuSgUQO3satWq0bp1az7//HN69uzJhAkTuOSSSxARypQpwwcffEDFihXZsmULbdq0oUePHlnuU/ziiy9Srlw55s+fz/z589OVvn7ooYeoWrUqycnJdO7cmfnz5zNo0CCeeuopJk+eTPXq1dOda9asWYwePZpp06ahqpxyyim0b9+eKlWqsGLFCsaPH8+rr77KxRdfzHvvvZft/ghXXHEFzz33HO3bt+eee+7hvvvu45lnnmHEiBGsXr2a0qVLp3ZZPfHEE4waNYq2bduyc+dOyuTzl5W3FMIMHQobNsCbbwYdiXMuXHgXUnjXkaoybNgw4uLi6NKlC7/99hsbN27M8jxTp05N/XKOi4sjLqzGzcSJE4mPj6dVq1YsWrTokMXufvjhBy644ALKly9PbGwsvXr14vvvvwegYcOGtGzZEsi+PDfY/g7btm2jffv2AFx55ZVMnTo1NcbLLruMsWPHpq6cbtu2LYMHD2bkyJFs27Yt31dUF6+WwqJFNoCQhU6doHVrW8x2zTVQxFavO5d3AdXOPv/88xk8eDCzZ89m9+7dqb/wx40bx+bNm5k1axYlS5akQYMGmZbLDpdZK2L16tU88cQTzJgxgypVqnDVVVcd8jzZ1Y1LKbsNVnr7UN1HWfnvf//L1KlTmTRpEg888ACLFi1iyJAhnHPOOXz66ae0adOGr7/+mhNOOOGwzp+Z4tNSGDMGTjwRpk3L8hARGDYMVq2CUHVc51wUiI2NpUOHDvzjH/9IN8C8fft2jjzySEqWLMnkyZNZu3Zttuc544wzGDduHAALFy5k/vz5gJXdLl++PJUqVWLjxo189tlnqc+pUKFCpv32Z5xxBh9++CF///03u3bt4oMPPuD000/P9XurVKkSVapUSW1lvPXWW7Rv354DBw6wbt06OnbsyGOPPca2bdvYuXMnv/zyCyeeeCJ33nknCQkJLF26NNevmZ3i81v4wgvhrrtswGDaNNuGLRPnnQdNm1pZ7T59IIt9wJ1zBaxv37706tUr3Uykyy67jPPOO4+EhARatmx5yF/M1113HVdffTVxcXG0bNmS1q1bA7aLWqtWrWjWrNlBZbcHDBjAWWedRa1atZg8eXLq/fHx8Vx11VWp5+jfvz+tWrXKtqsoK2+++SYDBw7k77//plGjRowePZrk5GT69evH9u3bUVVuueUWKleuzN13383kyZOJiYmhadOmqbvI5RtVjcgFeAPYBCzM4vETgJ+AvcBtOT3vSSedpIdt/HhVUH3xxWwPe+stO2zSpMN/KeeKisWLFwcdgsulzP5mwEzNwXdsJH8HjwG6Z/P4H8AgoOBWb1xyCXTsaH1EoSllmenTBxo0sI14Ctl2E845lycRSwqqOhX74s/q8U2qOgPYH6kYDiICzz9vK9SGDs3ysBIl4I47rJdpypQCi8455wJXKHrMRWSAiMwUkZmbN2/O28maNrW51q+9Bj//nOVhV18NRx0FDz+ct5dzrihQbzIXGnn9WxWKpKCqr6hqgqom1KhRI+8nvOceqF3bBp2TkzM9pEwZuPVW+PprmDEj7y/pXGFVpkwZtm7d6omhEFBVtm7dmqcFbcVn9lG4ChWsVnafPlbX4rrrMj1s4EBrKTzyCLz/fgHH6FyUqFu3LomJieS5le4KRJkyZahbt+5hP794JgWAiy+2hHDXXdC7N2TSAqlQAW680bbrXLzYep6cK25KlixJw4YNgw7DFZCIdR+JyHhsyunxIpIoIteIyEARGRh6vKaIJAKDgeGhYypGKp5MAszRoPOgQVCuHDz6aIFF5pxzgZHC1k+YkJCgh6p/nit33AGPPw4//QRt2mR6yODBVlZ75Uqbquqcc4WNiMxS1YRDHVcoBpoj6u67oU6dbAedBw+2lc2PP17AsTnnXAHzpJAy6Dx7dpabKdStC1deCa+/Dr//XsDxOedcAfKkAHDRRVYiddgwyGKGxR13wP79gRWJdM65AuFJAdIGnXfutA2bM9G4seWOF16wHdqcc64o8qSQokkTGzx44w0bdM7E0KE2WWnUqAKOzTnnCognhXCHGHRu0QLOOQeefRZ27QogPuecizBPCuFiY23Qec4cePnlTA9JKbD62msFHJtzzhUAX6eQkSp07QqzZsGyZXDkkQcd0r697c72yy9QqlTkQnHOufzi6xQOV8qg865dWQ46DxsGiYkwdmwBx+accxHmSSEzJ5xgg86jR2c66NytG8THw4gRWa53c865QsmTQlaGD7dVa9dff9A3v4jNRFqxAt57L6D4nHMuAjwpZCVl0HnuXHjppYMevuACOP54K61dyIZlnHMuS54UstO7N3TpYuW1N21K91BMjA05zJsHn38eUHzOOZfPPClkRwSeew7+/hvuvPOghy+9FOrV8y07nXNFhyeFQ0kZdB4zBn78Md1DpUrB7bfDDz/A998HE55zzuUnTwo5kTLo/M9/QlJSuoeuucY2bXvkkYBic865fORJISdiY+HppzMddC5XDm65BT77zBZCO+dcYeZJIacuvNBWOg8fDhs3pnvo+uuhYkVvLTjnCj9PCjkVPuicYaVzpUrWs/Tuu7B8eUDxOedcPvCkkBvHHw+33mqDzv/7X7qHbr4ZSpeGRx8NJjTnnMsPnhRya/hwm4eaYdD5yCOhf3/497/h118DjM855/LAk0JulS9vK53nzYMXX0z30G232X+ffDKAuJxzLh9ELCmIyBsisklEFmbxuIjISBFZKSLzRSQ+UrHku5RB57vvTjfofPTR0K8fvPpqlls9O+dcVItkS2EM0D2bx88CGocuA4AXszk2umSz0vnOO2HPHtudzTnnCpuIJQVVnQr8kc0hPYF/q/kZqCwitSIVT747/njrL3rzTVvSHHLCCdCrl23J8NdfAcbnnHOHIcgxhTrAurDbiaH7DiIiA0RkpojM3BxN/TJ33ZXpoPPQobB9+0FDDs45F/WCTAqSyX2ZFqFW1VdUNUFVE2rUqBHhsHKhfHlb6Tx/froMcNJJcOaZNh69e3eA8TnnXC4FmRQSgXpht+sC6wOK5fD16mVbsWVY6Tx0qFXbfuONAGNzzrlcCjIpTAKuCM1CagNsV9UNAcZzeFIGnXfvhjvuSL37jDPgtNPg8cdh//4A43POuVyI5JTU8cBPwPEikigi14jIQBEZGDrkU2AVsBJ4Fbg+UrFE3HHH2aDzv/+dOugsAsOGwdq1MH58wPE551wOiRayvSQTEhJ05syZQYdxsF27oEkTqFIFZs2CEiVQhZYtYd8+WLQIjvClgs65gIjILFVNONRx/jWVX8qXh2eesUHnF14ArLUwdCgsXQoffhhwfM45lwPeUshPqnDWWfDTT7BsGdSsSXKyrV2oXBmmT7dE4ZxzBc1bCkEQgZEj0w06x8TY1Zkz4euvA47POecOwZNCfjvuONu4+a23UjduvuIKqF0bHn444Nicc+4QPClEwrBhUL9+6krn0qVtctKUKdaz5Jxz0cqTQiSkDDovWACjRgFw7bVQrZpv2emci26eFCLl/POt1sU998CGDcTGwqBB8PHHNkHJOeeikSeFSElZ6bxnT+qg8w03QGwsjBgRcGzOOZcFTwqR1LixDTqPHQtTp1K1qg0zjB8Pr78edHDOOXcwTwqRFj7ovH8/991nvUrXXgsTJwYdnHPOpedJIdLKlbNB54ULYdQoSpeG99+Hdu3gssvg00+DDtA559J4UigI558P3bunDjqXK2cDznFxtt3zlClBB+icc8aTQkFIWem8d2/qoHOlSvDFF9CoEZx3npXAcM65oHlSKCiNG1tCCA06A1SvDl99BUceaQ2JBQsCjtE5V+x5UihIQ4fC0UfDP/5h2UCV2rWtJlLZstC1K6xYEXSQzrnizJNCQSpXDsaMsbUL3bpBmzbw8cc0bKB8/TUkJ0OXLrBuXdCBOueKK08KBa1DB/jlF3j5ZdvEuUcPaNWKJgv/wxefJrNtmyWGsO2enXOuwHhSCELp0jBgACxfDm++aS2Hiy8m/ormzBj0FhvWJdGtG/z5Z9CBOueKG08KQSpZ0upqL1oE77wDJUty3INX8Hvl4zl10Wv06L6PnTuDDtI5V5x4UogGMTFw8cUwdy58+CHl6lTlpeRreXv6Mbxx0vPs+XN30BE654oJTwrR5IgjoGdPW7Tw+eeUbnw0g5bfyO5aDUl+9Am82eCcizRPCtFIBM48kyOXfc+HN09h5t4TiRlyO3r00fDgg7BtW9AROueKqIgmBRHpLiLLRGSliAzJ5PEqIvKBiMwXkeki0jyS8RQ6Ipz/dHtmj/iKNvzEgtjT4O67ba3D8OGwZUvQETrnipiIJQURiQFGAWcBTYG+ItI0w2HDgLmqGgdcATwbqXgKszvvhE5D29Di1495qt9stFs32/D56KNtn88NG4IO0TlXRESypdAaWKmqq1R1HzAB6JnhmKbANwCquhRoICJHRTCmQuuhh6z69q1jW/Fg3H+s6mqvXvD009CwIdx4I/z6a9BhOucKuUgmhTpA+NrcxNB94eYBvQBEpDVwNFA344lEZICIzBSRmZs3b45QuNEtpabeFVdYsdVnv2oKb70Fy5ZBv37w0ktw7LG2UcMvvwQdrnOukIpkUpBM7tMMt0cAVURkLnAjMAdIOuhJqq+oaoKqJtSoUSP/Iy0kjjjCdmzr1QtuvhneeANLBK+9ZolgwABLFMcdB5dfDkuWBB2yc66QiWRSSATqhd2uC6wPP0BV/1LVq1W1JTamUANYHcGYCr0SJeDtt610Urrd2+rXh+efh9Wr4ZZbbCefZs3goots/YNzzuVAjpKCiBwjIqVD1zuIyCARqXyIp80AGotIQxEpBfQBJmU4b+XQYwD9gamq+lfu3kLxk7J722mnZbJ7W61a8MQTsHatbQX65ZfQqpVt2jBtWmAxO+cKh5y2FN4DkkXkWOB1oCHwdnZPUNUk4AbgC2AJMFFVF4nIQBEZGDqsCbBIRJZis5RuOoz3UCyVLw+ffJK2e9t332U4oHp1W9Owdi088AD8+KNVZe3aNZODnXPOiGrGbv5MDhKZrarxInI7sEdVnxOROaraKvIhppeQkKAzZ84s6JeNWlu2QPv2NvHom2+gdessDty50wajn3jCSrC2awe33w5nnWU1mJxzRZqIzFLVhEMdl9OWwn4R6QtcCXwSus+/SaJAyu5tNWocYve22Fhb07B6tU1jWrPGSmrUrm3TWadNgxz8QHDOFW05TQpXA6cCD6nqahFpCIyNXFguN3K1e1vZspYEfvkFJk2CTp3g1Veta+m44+C++3xKq3PFWI66j9I9QaQKUE9V50cmpOx591HWFi+2rqRy5eCHH6BevUM/B4Dt2+G992z/6ClTrMVw6qm2/uHii6054pwr1PK1+0hEpohIRRGpii04Gy0iT+U1SJe/mjaFL74gdfe2TZty+MRKlWzf6G+/tYHpRx+FHTtsCXWtWrY73MSJsNtLeDtX1OW0+6hSaKpoL2C0qp4EdIlcWO5wxcfbFNXERA5v97Z69eCOO2xwYt48WyU3axZccgnUrAnXXAOTJ8OBAxGJ3zkXrJwmhRIiUgu4mLSBZhel2raFDz6wBc1nn52HbRji4uDxx21q09df21LqiRNtHOLoo61SX5Yj2865wiinSeF+bL3BL6o6Q0QaAdkNZ7qAdesGEybAjBk2yWjPnjycLCYGOneG0aNtOuuECdCiBTz5pCWOFi0sefz2W77F75wLRq4HmoPmA82589ZbVkSvRw949918XpKwebPtLT12rE1pFbFWRL9+1qqoWDEfX8w5lxf5PdBcN7QZziYR2Sgi74nIQdVMXfS5/HIYNcpmn151FSQn5+PJa9SAG26An3+G5cutfOuaNXD11XDUUdCnD/z3v7B/fz6+qHMuknLafTQaq1tUGyt//XHoPlcIXH89PPKIFdK7/voIrVFr3BjuvdcWSfz0k81m+vprOPdcXyDnXCGS06RQQ1VHq2pS6DIGq2jqCokhQ2DoUHjlFZtcFLHvZhFbCDdqFKxf7wvknCtkcpoUtohIPxGJCV36AVsjGZjLfym7tz3xhF2PuFKlrDrrO+8Vk7UcAAAXOUlEQVTYAPXrr9uU1/vus30gTjsNXnjBxiacc1Ehp0nhH9h01N+BDUBvrPSFK0TCd2+7+25LDPk6xpCdQy2Q69rVWhNbthRQQM65zBz27CMRuVlVn8nneA7JZx/lXVKS7cMwcSIkJFjx1JNOCiiY+fOtJTFxIqxcadNfO3Wy8hoXXADVqgUUmHNFS35XSc3M4Dw81wWoRAlbajBuHKxbZ+W2Bw2yEkgFLi7OmizLl8OcOTbgsWqVbSt31FFw5pnW7bTVeyudKwh5SQqZ7cHsCgkRuPRSWLoUrrvOdvJs0sR+tAcyQUgEWraEhx+2GUyzZ9t+DytXQv/+VmKje3fbmPqPPwII0LniIS9JwecWFgGVK1tCmDbNuvb79LEf59mW3440EdtC9JFHLCnMmgW33mqtiWuusRbEWWfZCutcF3dyzmUn2zEFEdlB5l/+ApRV1RKRCiwrPqYQOcnJNhlo+HDYu9emsN55J5QpE3RkIarWgpg40S5r1lhfWNeuNgbRsydUqRJ0lM5FpZyOKXiZC3eQDRtg8GAbd2jc2JYcdO0adFQZqFoLIiVBrF1rNTzCE0TlykFH6VzUKIiBZldE1aoF48fDl1/ad2+3btC3ryWLqCFiU6cee8y2GJ0+HW66CRYutHoeRx5pq6n//W/bYMI5lyOeFFyWuna1ytj/+he8/z6ccIKNPxTY2oacEoGTT7ZKrWvW2ADJoEE23fXKKy1BnHeeVQcMZIqVc4WHdx+5HFmxwtaZffWVrWl46SX7oR7VVC1B/Oc/dlm3zlZZn3mmdTH16OGVXF2xERXdRyLSXUSWichKERmSyeOVRORjEZknIotExFdJR6nGjW2rz/HjbduE1q2tQGpU//BOqcP05JPWgvjxR8tsc+ZY+dgaNWzsYdw4+OuvoKN1LipErKUgIjHAcqArkAjMAPqq6uKwY4ZhW33eKSI1gGVATVXdl9V5vaUQvO3bbYbSCy9Yz8xTT9lUViksK1cOHLBy3yktiN9+g9KlrRbTGWdA+/aWTMqWDTpS5/JNNLQUWgMrVXVV6Et+AtAzwzEKVBARAWKBP4CkCMbk8kGlSvDccza2W7euLYLr1s2WERQKRxxhCeDpp22r0R9+sJri27bB/fdbmY1KlaBdO7jrLmsi7dgRdNTOFYhIthR6A91VtX/o9uXAKap6Q9gxFbB9Gk4AKgCXqOp/MznXAGAAQP369U9au3ZtRGJ2uZecbOMLw4bZlp8pJbqjZm1Dbm3bBv/7H3z3HUydCjNn2puMiYH4eGtFnHGGJQxfE+EKkcDXKYjIRcCZGZJCa1W9MeyY3kBbrI7SMcBXQAtVzbKD17uPotOGDbboePx4q4o9apS1Hgq9nTtt06CUJDFtGuzbZ31lcXFpSeKMM2yMwrkoFQ3dR4lAvbDbdYH1GY65GnhfzUpgNdZqcIVMrVq2s9tXX9n35ZlnwiWX2D47hVpsrM3NffBBSwrbtsGUKbbLXPXqVu67d28bXGna1ApJTZhQBN64K64i2VIogQ00dwZ+wwaaL1XVRWHHvAhsVNV7ReQoYDbWUsiyqL63FKLfnj22puzhh20G6IMP2qSfmJigI4uAfftsZXVKS+KHH9LGH449Nm3g+owzoEGDQEN1xVvg3UehIM4GngFigDdU9SERGQigqi+JSG1gDFALq6c0QlXHZndOTwqFx8qVlgy+/NK64196ydaYFWlJSTBvXlqSmDo1rWhf/frpk0TjxoVoypYr7KIiKUSCJ4XCRdVmfd58M/z+u/WuPPRQMSpLdOAALFqUliS++w42bbLHatZMnySaNrWZUc5FgCcFF1X++su2AH3+eRuPffJJm8pa7H4oq9rc3fAkkZhoj1WrBqefbmskWre2peO+4trlE08KLirNng0DB8KMGbYc4IUX4Pjjg44qQKq22jolSXz/vfW7gWXMJk3glFMsSbRuDSeeaNVgncslTwouaiUnwyuv2HqG3butdt3tt9sEHodtPTpzpk1/nT7dLps322NlytgGRClJonVrOOaYYtjkcrnlScFFvd9/t018xo6177obboDbbvPp/gdRtf0iUhLE9Ok24+nvv+3xqlXTJ4mTT/YM6w7iScEVGkuX2rTV8eM9OeRYUhIsXmwJIqVFsXChDWyDTX8NTxTx8VC+fKAhu2B5UnCFTkpyePttq0XnySGXdu2yQZvwFsWaNfZYTAw0b54+UTRtatuZumLBk4IrtJYuhQcesJZDuXKWHG691ZPDYdm0yUb1w8cnUtZNlCtnM5xSksQpp9haCh+fKJI8KbhCb8mStG6llORw221WXcIdJlX45Zf0rYnZs2HvXnv8yCPTxiVatbJLnTqeKIoATwquyMiYHG680VoOnhzyyb59Nh4R3ppYssQSCNgHnZIgWra0/zZuXETrlhRdnhRckbNkiXUrTZjgySHidu60ch1z59pOdXPmWOLYF9r/qnx5qxKbkixatYJmzQpxzfSiz5OCK7I8OQRk3z778FOSxJw5ljRSCgCWKGGL7cITRcuWtmGRC5wnBVfkhSeH8uUtOQwe7MmhQB04AKtXp08Uc+bYIpQUjRqldTulXGrV8nGKAuZJwRUbixdbcnjnHU8OUeP339O3JubMSSvfATagHT5G0aqVlRr3goAR40nBFTuZJYdbb7U6cy4K/PWXjVOEtygWLbKFeGAbGrVokT5ZNGsGpUsHG3cR4UnBFVsZk8OgQdZy8OQQhfbutcQQPqA9b54NdIPNcGrUyMYqmjSBE05I+6+PVeSKJwVX7C1aZMlh4kRPDoXKgQPW1TRnDixYYKsZlyyBFStg//6042rXTksS4UnDxysy5UnBuRBPDkXE/v02qL1kSdolJWGkzIACa0GEtyhSEkbDhsW6rIcnBecyCE8OsbFpyaFq1aAjc3miCuvXpyWI8ISxYUPacaVK2aK7jC2L44+3uc1FnCcF57KwcKElh//8x5NDkbdtW1qyCE8aq1alVZQVgaOPPrhl0aRJkWpOelJw7hAyJodrrrFd4Yr1TnDFxd69NkaRMVksW2Y7P6WoXt2Sw7HHQr16dqlfP+16bGxw7yGXPCk4l0MLF8Ijj1hy2L8fOneG66+HHj2KdRd08XTgAPz668FjFqtWWVdUxu/LypXTJ4mMiaNOnaiZUutJwblc2rgRXn8dXn7Zvhfq1IFrr7VL7dpBR+cCt3+/jV2sW2f/g6xbl/7y66+2lWpGRx11cOIIv12zZoEUF4yKpCAi3YFngRjgNVUdkeHx24HLQjdLAE2AGqr6R1bn9KTgIi05GT79FF54Ab74whbZnn++tR46dvTZji4bf/8NiYmZJ46U2ylrMFKUKGG/QLJqbdSrZ2MbefwfL/CkICIxwHKgK5AIzAD6quriLI4/D7hFVTtld15PCq4g/fKLtRxefx3++MPGIQcOhCuvtJ4D53JFFbZvz761kZiYVo02Rdmylhyuvx5uuumwXjoaksKpwL2qembo9lAAVX0ki+PfBiar6qvZndeTggvCnj025vDCC/Dzz/Zv9LLL4LrrbPtj5/LNgQOweXPmiePcc+Hyyw/rtNGQFHoD3VW1f+j25cApqnpDJseWw1oTx2bWdSQiA4ABAPXr1z9p7dq1EYnZuZyYMwdefBHGjbPeglNOsR9wF1/s2wm46JXTpBDJkoSZdYBllYHOA/6X1ViCqr6iqgmqmlDDN+p1AWvVCl55BX77DUaOtN6AK6+0buHbb7cuJ+cKq0gmhUSgXtjtusD6LI7tA4yPYCzO5bvKla0S6+LF8O23NpX1mWdsSnv37jBpkg1aO1eYRDIpzAAai0hDESmFffFPyniQiFQC2gMfRTAW5yJGxGYlTZwIa9fCfffZ2oeePa3czkMPpd9zxrloFrGkoKpJwA3AF8ASYKKqLhKRgSIyMOzQC4AvVXVXpGJxrqDUrg333ANr1sD779vq6OHDbeJInz4wderB65+ciya+eM25CFu+HF56CUaPtlI8zZrZrKXLL4eKFYOOzhUX0TDQ7JwDjjsOnnrKBqbfeMOms95wg7UqBg60PWWcixaeFJwrIOXKwdVXw4wZMH26TWF9803bebJdO5viundv0FG64s6TgnMBOPlkazX89pu1IjZtgn79oG5d6N8fPv44fbFO5wqKJwXnAlS1KtxyixXj/PJL6NLFVk736GFVmy+4AMaMgS1bgo7UFRdeGNi5KHDEEdC1q1327YMpU2ydw0cfwYcf2uOnnWbTXHv2tA3EnIsEn33kXBRTtbIaH31kl5RB6SZNLDn06GFlNo7wNr87hMBrH0WKJwVXnK1dm9aC+O47SEqycv3nnWdJonNnm93kXEaeFJwr4v78Ez77zBLEZ5/Bjh02w+nMM60Fce65Ni7hHHhScK5Y2bvXWg4p3Uy//WZdSm3bpo1DHHts0FG6IHlScK6YUoXZs9MSxPz5dn/TptaC6NkTWrf2cYjixpOCcw6A1attHGLSJGtNJCfbtsAp4xCdOvk4RHHgScE5d5A//7T9p1PGIXbuTBuH6NkTzjnHxyGKKk8Kzrls7d0Lkydbgpg0Cdavty6ldu0sQfTubXvHu6LBk4JzLsdUYdastHGIBQvs/jZtrEZT795W/tsVXp4UnHOHbeVKK7fxn//Y4jmAU09NSxB16wYbn8s9L53tnDtsxx4LQ4faLKbly233uN27rU5TvXo21fXZZ23qqytaPCk457LVuDEMG2YthmXL4MEHYdcuuPlmazG0awcjR3qCKCo8KTjncuy44+Cuu2DuXKvs+sADtpL6ppssQZx+Ojz3nA1au8LJxxScc3m2dGnaGMSCBSBiLYiLL4YLL4RatYKO0PlAs3MuEEuWpCWIhQstQZx+elqCqFkz6AiLJ08KzrnApSSIiRNh0SJLEGecYQmiVy9PEAXJk4JzLqosXmwJ4p13LFkccUT6BHHUUUFHWLR5UnDORa1Fi6z1MHGijUcccQS0b5+WII48MugIi56oWKcgIt1FZJmIrBSRIVkc00FE5orIIhH5LpLxOOeiQ7NmcN991npYsMBmNK1fD9ddZ4PSnTvDyy/Dxo1BR1r8RKylICIxwHKgK5AIzAD6qurisGMqAz8C3VX1VxE5UlU3ZXdebyk4VzSp2sB0Sgti+XK7/8QTLUl06mStiYoVg42zsAq8+0hETgXuVdUzQ7eHAqjqI2HHXA/UVtXhOT2vJwXnij5Va0H897/w7bfwww+wZw/ExEBCQlqSOO00L/udU9HQfVQHWBd2OzF0X7jjgCoiMkVEZonIFZmdSEQGiMhMEZm5efPmCIXrnIsWIhAXZ6U2vvrKSn5Pnmy3Y2Lg0UehSxeoUsUSxEMPwc8/257VLm8i2VK4CDhTVfuHbl8OtFbVG8OOeR5IADoDZYGfgHNUdXlW5/WWgnNuxw74/nv45htrScyda/dXqGBdTJ06WbJo3tx3mEuR05ZCiQjGkAiEF9utC2Rc/J4IbFHVXcAuEZkKtMDGIpxzLlMVKsDZZ9sFYMsWmDLFksQ338Ann9j9NWpAx45pSeKYY6wV4rIWyZZCCezLvTPwGzbQfKmqLgo7pgnwPHAmUAqYDvRR1YVZnddbCs65Q1m3zloQ335rSSKlWF/9+pYgUpJE7drBxlmQAm8pqGqSiNwAfAHEAG+o6iIRGRh6/CVVXSIinwPzgQPAa9klBOecy4l69eDKK+2iCitWpHU1ffwxjBljx51wQlqC6NABqlYNMuro4IvXnHPFyoEDMH9+WlfT1KlWClwEWrVKSxLt2kFsbNDR5p/Ap6RGiicF51x+2r8fpk9P62r66SfYtw9KlLDtSFO6m045BcqUCTraw+dJwTnnDsPff8P//peWJGbNstZFmTK2JWnHjtbV1Lo1lC4ddLQ550nBOefywbZtNv118mS7zJtn4xRly9riuZQkcfLJUKpU0NFmzZOCc85FwB9/2DjElCmWJObPt/vLlbO9q1OSREIClCwZZKTpeVJwzrkCsHUrfPddWpJYGJo/Wb68DVanJImTTrJxiqB4UnDOuQBs3pw+SSwOlQCNjbUd6FKSRKtWBZskPCk451wU2LgxfZJYutTur1gxfZJo2dLqOkWKJwXnnItCv/9uCSIlSaSUCK9UyXaiS0kSLVrkb90mTwrOOVcIrF+fPkmsXGn3V6mSPkmceGLekoQnBeecK4QSE9MSxJQpsGqV3V+1qu1QN3jw4Z038NpHzjnncq9uXejXzy4Av/6aliQKooCftxScc64YiIad15xzzhUynhScc86l8qTgnHMulScF55xzqTwpOOecS+VJwTnnXCpPCs4551J5UnDOOZeq0C1eE5HNwNqg48ij6sCWoIOIIv55pOefRxr/LNLLy+dxtKrWONRBhS4pFAUiMjMnKwuLC/880vPPI41/FukVxOfh3UfOOedSeVJwzjmXypNCMF4JOoAo459Hev55pPHPIr2Ifx4+puCccy6VtxScc86l8qTgnHMulSeFAiQi9URksogsEZFFInJT0DEFTURiRGSOiHwSdCxBE5HKIvKuiCwN/T9yatAxBUlEbgn9O1koIuNFpEzQMRUkEXlDRDaJyMKw+6qKyFcisiL03yr5/bqeFApWEnCrqjYB2gD/FJGmAccUtJuAJUEHESWeBT5X1ROAFhTjz0VE6gCDgARVbQ7EAH2CjarAjQG6Z7hvCPCNqjYGvgndzleeFAqQqm5Q1dmh6zuwf/R1go0qOCJSFzgHeC3oWIImIhWBM4DXAVR1n6puCzaqwJUAyopICaAcsD7geAqUqk4F/shwd0/gzdD1N4Hz8/t1PSkEREQaAK2AacFGEqhngDuAA0EHEgUaAZuB0aHutNdEpHzQQQVFVX8DngB+BTYA21X1y2CjigpHqeoGsB+ZwJH5/QKeFAIgIrHAe8DNqvpX0PEEQUTOBTap6qygY4kSJYB44EVVbQXsIgJdA4VFqK+8J9AQqA2UF5F+wUZVPHhSKGAiUhJLCONU9f2g4wlQW6CHiKwBJgCdRGRssCEFKhFIVNWUluO7WJIorroAq1V1s6ruB94HTgs4pmiwUURqAYT+uym/X8CTQgESEcH6jJeo6lNBxxMkVR2qqnVVtQE2gPitqhbbX4Kq+juwTkSOD93VGVgcYEhB+xVoIyLlQv9uOlOMB97DTAKuDF2/Evgov1+gRH6f0GWrLXA5sEBE5obuG6aqnwYYk4seNwLjRKQUsAq4OuB4AqOq00TkXWA2NmtvDsWs5IWIjAc6ANVFJBH4FzACmCgi12CJ86J8f10vc+Gccy6Fdx8555xL5UnBOedcKk8KzjnnUnlScM45l8qTgnPOuVSeFJwLEZFkEZkbdsm3FcUi0iC82qVz0crXKTiXZreqtgw6COeC5C0F5w5BRNaIyKMiMj10OTZ0/9Ei8o2IzA/9t37o/qNE5AMRmRe6pJRniBGRV0N7BHwpImVDxw8SkcWh80wI6G06B3hScC5c2QzdR5eEPfaXqrYGnsequxK6/m9VjQPGASND948EvlPVFlj9okWh+xsDo1S1GbANuDB0/xCgVeg8AyP15pzLCV/R7FyIiOxU1dhM7l8DdFLVVaGChr+rajUR2QLUUtX9ofs3qGp1EdkM1FXVvWHnaAB8FdocBRG5Eyipqg+KyOfATuBD4ENV3Rnht+pclryl4FzOaBbXszomM3vDrieTNqZ3DjAKOAmYFdpUxrlAeFJwLmcuCfvvT6HrP5K2ReRlwA+h698A10HqHtQVszqpiBwB1FPVydiGQ5WBg1orzhUU/0XiXJqyYdVrwfZLTpmWWlpEpmE/pPqG7hsEvCEit2O7pqVUNb0JeCVUyTIZSxAbsnjNGGCsiFQCBHjat+F0QfIxBecOITSmkKCqW4KOxblI8+4j55xzqbyl4JxzLpW3FJxzzqXypOCccy6VJwXnnHOpPCk455xL5UnBOedcqv8H2j2HawNDo1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# #Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "# This script demonstrates how to implement a basic character-level\n",
    "# sequence-to-sequence model. We apply it to translating\n",
    "# short English sentences into short French sentences,\n",
    "# character-by-character. Note that it is fairly unusual to\n",
    "# do character-level machine translation, as word-level\n",
    "# models are more common in this domain.\n",
    "\n",
    "# **Summary of the algorithm**\n",
    "\n",
    "# - We start with input sequences from a domain (e.g. English sentences)\n",
    "#     and corresponding target sequences from another domain\n",
    "#     (e.g. French sentences).\n",
    "# - An encoder LSTM turns input sequences to 2 state vectors\n",
    "#     (we keep the last LSTM state and discard the outputs).\n",
    "# - A decoder LSTM is trained to turn the target sequences into\n",
    "#     the same sequence but offset by one timestep in the future,\n",
    "#     a training process called \"teacher forcing\" in this context.\n",
    "#     It uses as initial state the state vectors from the encoder.\n",
    "#     Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "#     given `targets[...t]`, conditioned on the input sequence.\n",
    "# - In inference mode, when we want to decode unknown input sequences, we:\n",
    "#     - Encode the input sequence into state vectors\n",
    "#     - Start with a target sequence of size 1\n",
    "#         (just the start-of-sequence character)\n",
    "#     - Feed the state vectors and 1-char target sequence\n",
    "#         to the decoder to produce predictions for the next character\n",
    "#     - Sample the next character using these predictions\n",
    "#         (we simply use argmax).\n",
    "#     - Append the sampled character to the target sequence\n",
    "#     - Repeat until we generate the end-of-sequence character or we\n",
    "#         hit the character limit.\n",
    "\n",
    "# **Data download**\n",
    "\n",
    "# [English to French sentence pairs.\n",
    "# ](http://www.manythings.org/anki/fra-eng.zip)\n",
    "\n",
    "# [Lots of neat sentence pairs datasets.\n",
    "# ](http://www.manythings.org/anki/)\n",
    "\n",
    "# **References**\n",
    "\n",
    "# - [Sequence to Sequence Learning with Neural Networks\n",
    "#    ](https://arxiv.org/abs/1409.3215)\n",
    "# - [Learning Phrase Representations using\n",
    "#     RNN Encoder-Decoder for Statistical Machine Translation\n",
    "#     ](https://arxiv.org/abs/1406.1078)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_characters\n",
    "# input_characters\n",
    "# input_texts\n",
    "# output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 10000  # Number of samples to train on.\n",
    "\n",
    "# # Vectorize the data.\n",
    "# input_texts = []\n",
    "# target_texts = []\n",
    "# input_characters = set()\n",
    "# target_characters = set()\n",
    "\n",
    "# with open('../data/text/rus-eng/rus.txt', 'rt', encoding='utf-8') as f:\n",
    "#     lines = f.read().split('\\n')\n",
    "    \n",
    "# for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "#     input_text, target_text = line.split('\\t')\n",
    "#     # We use \"tab\" as the \"start sequence\" character\n",
    "#     # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     target_text = '\\t' + target_text + '\\n'\n",
    "#     input_texts.append(input_text)\n",
    "#     target_texts.append(target_text)\n",
    "#     for char in input_text:\n",
    "#         if char not in input_characters:\n",
    "#             input_characters.add(char)\n",
    "#     for char in target_text:\n",
    "#         if char not in target_characters:\n",
    "#             target_characters.add(char)\n",
    "\n",
    "# input_characters = sorted(list(input_characters))\n",
    "# target_characters = sorted(list(target_characters))\n",
    "# num_encoder_tokens = len(input_characters)\n",
    "# num_decoder_tokens = len(target_characters)\n",
    "# max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "#     for t, char in enumerate(input_text):\n",
    "#         encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "#     for t, char in enumerate(target_text):\n",
    "#         # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "#         decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "#         if t > 0:\n",
    "#             # decoder_target_data will be ahead by one timestep\n",
    "#             # and will not include the start character.\n",
    "#             decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next: inference mode (sampling).\n",
    "# # Here's the drill:\n",
    "# # 1) encode input and retrieve initial decoder state\n",
    "# # 2) run one step of decoder with this initial state\n",
    "# # and a \"start of sequence\" token as target.\n",
    "# # Output will be the next target token\n",
    "# # 3) Repeat with the current target token and current states\n",
    "\n",
    "# # Define sampling models\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq_index in range(100):\n",
    "#     # Take one sequence (part of the training set)\n",
    "#     # for trying out decoding.\n",
    "#     input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "#     print('-')\n",
    "#     print('Input sentence:', input_texts[seq_index])\n",
    "#     print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
