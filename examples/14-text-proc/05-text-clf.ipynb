{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**извлечение признаков из текста на естественном языке**\n",
    "\n",
    "классификатор текстов\n",
    "\n",
    "частотный анализ с очисткой стоп-слов (TF)\n",
    "\n",
    "Евгений Борисов borisov.e@solarl.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 3196\n"
     ]
    }
   ],
   "source": [
    "# загружаем тексты\n",
    "data = pd.read_pickle('../data/text/news.pkl')\n",
    "print('записей:',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>В Минске не проданы квартиры в пяти десятках сданных и строящихся ЖК\\n\\n27 ноября 2016 в 13:42\\n\\nИрина Осипова, REALTY.TUT.BY\\n\\nВласти неоднократно заявляли, что строительство в Минске будет огр...</td>\n",
       "      <td>realty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Геморрагическая лихорадка продолжает наступление на Оренбуржье В Оренбургской\\nобласти продолжается высокий сезонный подъем заболеваемости геморрагической\\nлихорадкой с почечным синдромом. На 15 н...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         text  \\\n",
       "1196  В Минске не проданы квартиры в пяти десятках сданных и строящихся ЖК\\n\\n27 ноября 2016 в 13:42\\n\\nИрина Осипова, REALTY.TUT.BY\\n\\nВласти неоднократно заявляли, что строительство в Минске будет огр...   \n",
       "336   Геморрагическая лихорадка продолжает наступление на Оренбуржье В Оренбургской\\nобласти продолжается высокий сезонный подъем заболеваемости геморрагической\\nлихорадкой с почечным синдромом. На 15 н...   \n",
       "\n",
       "         tag  \n",
       "1196  realty  \n",
       "336   health  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CountVectorizer + TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Stemmer import Stemmer\n",
    "# pacman -S python-pystemmer\n",
    "# pip install pystemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= [\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', \n",
    "    'так', 'его', 'но',  'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', \n",
    "    'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', \n",
    "    'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или',  'ни', 'быть', 'был', 'него', 'до', \n",
    "    'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', \n",
    "    'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем',\n",
    "    'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', \n",
    "    'ж', 'тогда', 'кто','этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', \n",
    "    'этом', 'один', 'почти', 'мой', 'тем','чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем',\n",
    "    'всех', 'никогда', 'можно', 'при', 'наконец', 'два','об', 'другой', 'хоть', 'после', \n",
    "    'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них','какая', 'много', \n",
    "    'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда',\n",
    "    'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', \n",
    "    'всегда', 'конечно', 'всю', 'между']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    tt = [ t for t in text.split() if t ]\n",
    "    # tt = [ t.lower()  for t in tt ] # приведение в lowercase\n",
    "    tt = [ re.sub( r'https?://[\\S]+', 'url', t)  for t in tt ]  # замена интернет ссылок\n",
    "    tt = [ re.sub( r'[\\w\\./]+\\.[a-z]+', 'url', t) for t in tt  ]  # замена интернет ссылок \n",
    "    tt = [ re.sub( r'<[^>]*>', '', t)  for t in tt ] # удаление html тагов\n",
    "    tt = [ re.sub( r'\\W', '', t)  for t in tt ] # удаление лишних символов (НЕ буква и НЕ цифра)\n",
    "    tt = [ t for t in tt if t not in stopwords ] # удаление (предлогов)\n",
    "    tt = Stemmer('russian').stemWords( tt ) # стемминг, выделение основы слова\n",
    "    tt = [ re.sub( r'\\b\\d+\\b', 'digit', t ) for t in tt ] # замена цифр\n",
    "    tt = [ t for t in tt if len(t)>2 ] # удаление коротких слов\n",
    "    return ' '.join( [ t.strip() for t in tt if t ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preprocessor at 0x7f93995b3bf8>,\n",
       "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=False, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"ручная\" очистка стоп-слов\n",
    "tf = TfidfVectorizer(\n",
    "    preprocessor=preprocessor,\n",
    "    lowercase=False,\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    norm='l2')\n",
    "\n",
    "tf.fit( data['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## формируем датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3196, 44173)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.transform( data['text'] )\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto': 0,\n",
       " 'culture': 1,\n",
       " 'economics': 2,\n",
       " 'health': 3,\n",
       " 'incident': 4,\n",
       " 'politics': 5,\n",
       " 'realty': 6,\n",
       " 'reclama': 7,\n",
       " 'science': 8,\n",
       " 'social': 9,\n",
       " 'sport': 10,\n",
       " 'tech': 11,\n",
       " 'woman': 12}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = { t:i for i,t in enumerate(sorted(set(data['tag']))) }\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 1, ..., 8, 5, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['tag'].map(labels).values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def get_seed(): t = time() ; return int(((t%1)/(t//1))*1e11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((319, 44173), (319,), (2877, 44173), (2877,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.9, random_state=get_seed() )\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='hinge',max_iter=1000)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тестируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754257907542579"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( classification_report(y_test,o) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import itertools\n",
    "\n",
    "# classes = sorted(labels.keys())\n",
    "# cm = confusion_matrix(y_test,o)\n",
    "# tick_marks = np.arange(len(classes))\n",
    "\n",
    "# plt.figure(figsize=(10,9))\n",
    "\n",
    "# plt.xticks(tick_marks, classes, rotation=45)\n",
    "# plt.yticks(tick_marks, classes)\n",
    "\n",
    "# thresh = cm.max() / 2.\n",
    "# for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#     plt.text(j, i, format(cm[i, j], 'd'),\n",
    "#              horizontalalignment=\"center\",\n",
    "#              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# plt.title('Confusion matrix')\n",
    "# plt.tight_layout()\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n",
    "# plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_inv = { labels[k]:k for k in labels}\n",
    "# labels_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag: auto\n",
      "predict: auto\n",
      "- - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Новое поколение Q3 копания Audi строит, что вполне ожидаемо, на модульной платформе MQB концерна Volkswagen. Как утверждает издание Auto Bild, в Ингольштадте собираются воспользоваться всеми преимуществами платформы, в которые, помимо простоты разработки и возможности увеличить пространство салона, входит и шанс использовать трехцилиндровые двигатели.\n",
      "\n",
      "Audi научились разговаривать со светофорами...\n",
      "\n",
      "По данным издания, Audi Q3 получит как минимум один такой мотор, но пока неизвестно, какой именно – по предварительным сведениям, это будет либо бензиновый турбированный двигатель объемом 1,0 л, либо 1,4-литровый дизель. Впрочем, не исключено, что в Audi решатся предложить кроссоверу оба мотора.\n",
      "\n",
      "Получит новая Audi Q3 и два варианта топового 2,5-литрового двигателя. Самая \"заряженная\" версия кроссовера под названием Q3 RS будет оснащаться мотором мощностью порядка 400 л.с., но в Audi впервые задумались о выпуске \"промежуточной\" S-модификации. Ее, скорее всего, снабдят тем же мотором, но в дефорсированной до 340 л.с. версии.\n",
      "\n",
      "Наконец, обещаны вседорожнику и гибридные силовые установки с возможностью подзарядки батарей от сторонних источников электричества, но основным рынком для таких Q3 станет Китай.\n",
      "\n",
      "Дату предполагаемого дебюта нового поколения Q3 в Ингольштадте пока не называют.\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(data))\n",
    "print('tag:',data.iloc[i,1])\n",
    "print('predict:',labels_inv[o[i]])\n",
    "print('- - - - - - - - - - - - - - - - - - \\n')\n",
    "print(data.iloc[i,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor(text):\n",
    "#     tt = [ t for t in text.split() if t ]\n",
    "#     tt = [ re.sub( r'https?://[\\S]+', 'url', t)  for t in tt ]  # замена интернет ссылок\n",
    "#     tt = [ re.sub( r'[\\w\\./]+\\.[a-z]+', 'url', t) for t in tt  ]  # замена интернет ссылок \n",
    "#     tt = [ re.sub( r'<[^>]*>', '', t)  for t in tt ] # удаление html тагов\n",
    "#     tt = [ re.sub( r'\\W', '', t)  for t in tt ] # удаление лишних символов (НЕ буква и НЕ цифра)\n",
    "#     tt = [ re.sub( r'\\b\\d+\\b', 'digit', t ) for t in tt ] # замена цифр\n",
    "#     return ' '.join( [ t.strip() for t in tt if t ] )\n",
    "    \n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# stemmer = Stemmer('russian')\n",
    "# analyzer = CountVectorizer().build_analyzer()\n",
    "# def stemmed_words(doc): return [ Stemmer('russian').stemWord(w) for w in analyzer(doc) ]\n",
    "# vect = CountVectorizer(lowercase=True,preprocessor=preprocessor,analyzer=stemmed_words)\n",
    "# vect.fit( data['text'] )\n",
    "\n",
    "# # размер словаря\n",
    "# print(len(vect.vocabulary_))\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tf = TfidfTransformer(use_idf=False, norm='l2')\n",
    "# data_vect = vect.transform( data['text'] )\n",
    "# tf.fit( data_vect )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
