{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**классификатор текстов LSTM на Keras+TensorFlow**\n",
    "\n",
    "Евгений Борисов <borisov.e@solarl.ru>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/company/dca/blog/274027/    \n",
    "http://help.sentiment140.com/for-students/   \n",
    "http://study.mokoron.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 200  \n",
    "import re\n",
    "# import gzip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/tqdm/std.py:699: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(d): return \"{:,.0f}\".format(d).replace(\",\", \" \")\n",
    "def ppr(d): print('записей:', pp(len(d)) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ю. В. Рубцова. Построение корпуса текстов для настройки тонового классификатора // Программные продукты и системы, 2015, №1(109), –С.72-78\n",
    "\n",
    "http://study.mokoron.com\n",
    "\n",
    "– id: уникальный номер сообщения в системе twitter;\n",
    "– tdate: дата публикации сообщения (твита);\n",
    "– tmane: имя пользователя, опубликовавшего сообщение;\n",
    "– ttext:  текст сообщения (твита);\n",
    "– ttype: поле в котором в дальнейшем будет указано к кому классу относится твит (положительный, отрицательный, нейтральный);\n",
    "– trep: количество реплаев к данному сообщению. В настоящий момент API твиттера не отдает эту информацию;\n",
    "– tfav: число сколько раз данное сообщение было добавлено в избранное другими пользователями;\n",
    "– tstcount: число всех сообщений пользователя в сети twitter;\n",
    "– tfol: количество фоловеров пользователя (тех людей, которые читают пользователя);\n",
    "– tfrien: количество друзей пользователя (те люди, которых читает пользователь);\n",
    "– listcount: количество листов-подписок в которые добавлен твиттер-пользователь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = ['id', 'tdate', 'tmane', 'ttext', 'ttype', 'trep', 'tfav', 'tstcount', 'tfol', 'tfrien', 'listcount','unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 111 923\n"
     ]
    }
   ],
   "source": [
    "neg = pd.read_csv('../data/twit/negative.csv.gz',sep=';',header=None)\n",
    "ppr(neg)\n",
    "neg.columns = ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 114 910\n"
     ]
    }
   ],
   "source": [
    "pos = pd.read_csv('../data/twit/positive.csv.gz',sep=';')\n",
    "ppr(pos)\n",
    "pos.columns = ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 226 833\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([pos,neg],sort=False)[['id','ttext', 'ttype']]\n",
    "ppr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ttext</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53060</th>\n",
       "      <td>410062212502667264</td>\n",
       "      <td>Да, не спорю ;) грозного взгляда инструктора же не будет \"RT @arzik2: @M_lleBarankina :)) http://t.co/M4Q6qbIeY1\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97589</th>\n",
       "      <td>422810176802942976</td>\n",
       "      <td>омг, яникс в Минске, лучше бы Билли приехал(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>409434898890358784</td>\n",
       "      <td>@fixmelater так ты с нами из-за уборки не идешь? давай. нам нужен четвертый человек)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57607</th>\n",
       "      <td>416097158811885568</td>\n",
       "      <td>RT @kalinina1907: ну к чему опять эти сны,а?:(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78749</th>\n",
       "      <td>410724034448338945</td>\n",
       "      <td>Думаю, каникулы чинуш в Куршевелях уже испорчены: невозможно же будет даже расслабиться - а вдруг за тобой наблюдает журналист Навального? )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91850</th>\n",
       "      <td>422020780197564416</td>\n",
       "      <td>@katya_colfer очень. в школе уроки сложные были, а потом пошли в студию фоткаться на альбоом((</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73109</th>\n",
       "      <td>418307802780532736</td>\n",
       "      <td>RT @russian_loser: @PastorAfuckingA Это кто еще БОЯТЬ чью жизнь пиздит :|</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104247</th>\n",
       "      <td>423970538617581568</td>\n",
       "      <td>RT @aracetiruo: Кстати, в отпуске прочитал, как   описал вампирский сериальчик. Даже посмотреть хочется, а то от BSG рвотные позывы :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79253</th>\n",
       "      <td>419440884590313472</td>\n",
       "      <td>37.5,потом 38.5,пойдёт выше-скорую вызывать будем:|</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31145</th>\n",
       "      <td>409651639407378432</td>\n",
       "      <td>RT @elkaundead: а снега то ещё больше мимими)))\\nМатвей держись)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id  \\\n",
       "53060   410062212502667264   \n",
       "97589   422810176802942976   \n",
       "22537   409434898890358784   \n",
       "57607   416097158811885568   \n",
       "78749   410724034448338945   \n",
       "91850   422020780197564416   \n",
       "73109   418307802780532736   \n",
       "104247  423970538617581568   \n",
       "79253   419440884590313472   \n",
       "31145   409651639407378432   \n",
       "\n",
       "                                                                                                                                               ttext  \\\n",
       "53060                              Да, не спорю ;) грозного взгляда инструктора же не будет \"RT @arzik2: @M_lleBarankina :)) http://t.co/M4Q6qbIeY1\"   \n",
       "97589                                                                                                   омг, яникс в Минске, лучше бы Билли приехал(   \n",
       "22537                                                           @fixmelater так ты с нами из-за уборки не идешь? давай. нам нужен четвертый человек)   \n",
       "57607                                                                                                 RT @kalinina1907: ну к чему опять эти сны,а?:(   \n",
       "78749   Думаю, каникулы чинуш в Куршевелях уже испорчены: невозможно же будет даже расслабиться - а вдруг за тобой наблюдает журналист Навального? )   \n",
       "91850                                                 @katya_colfer очень. в школе уроки сложные были, а потом пошли в студию фоткаться на альбоом((   \n",
       "73109                                                                      RT @russian_loser: @PastorAfuckingA Это кто еще БОЯТЬ чью жизнь пиздит :|   \n",
       "104247        RT @aracetiruo: Кстати, в отпуске прочитал, как   описал вампирский сериальчик. Даже посмотреть хочется, а то от BSG рвотные позывы :(   \n",
       "79253                                                                                            37.5,потом 38.5,пойдёт выше-скорую вызывать будем:|   \n",
       "31145                                                                               RT @elkaundead: а снега то ещё больше мимими)))\\nМатвей держись)   \n",
       "\n",
       "        ttype  \n",
       "53060       1  \n",
       "97589      -1  \n",
       "22537       1  \n",
       "57607      -1  \n",
       "78749       1  \n",
       "91850      -1  \n",
       "73109      -1  \n",
       "104247     -1  \n",
       "79253      -1  \n",
       "31145       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## токенизация и очистка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем словарь из текстов\n",
    "def get_vocabulary(ds):\n",
    "    vcb = [ set(s) for s in ds.tolist() ]\n",
    "    return sorted(set.union(*vcb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация и очистка с помощью пакета морфологического анализа\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "# применяет список замен pat к строке s\n",
    "def replace_patterns(s,pat):\n",
    "    if len(pat)<1: return s\n",
    "    return  replace_patterns( re.sub(pat[0][0],pat[0][1],s), pat[1:] )\n",
    "\n",
    "# нормализация текста\n",
    "def string_normalizer(s):\n",
    "    pat = [\n",
    "       [r'ё','е'] # замена ё для унификации\n",
    "       ,[r'</?[a-z]+>',' '] # удаляем xml\n",
    "       ,[r'[^a-zа-я\\- ]+',' '] # оставляем только буквы, пробел и -\n",
    "       ,[r' -\\w+',' '] # удаляем '-й','-тый' и т.п.\n",
    "       ,[r'\\w+- ',' ']\n",
    "       ,[r' +',' '] # удаляем повторы пробелов\n",
    "    ]\n",
    "    return replace_patterns(s.lower(),pat).strip()\n",
    "\n",
    "# NOUN (существительное), VERB (глагол), ADJF (прилагательное)\n",
    "def word_normalizer(w, pos_types=('NOUN','VERB','ADJF')):\n",
    "    if not morph.word_is_known(w): return ''\n",
    "    p = morph.parse(w)[0] \n",
    "    return p.normal_form if (p.tag.POS in pos_types) else ''\n",
    "\n",
    "\n",
    "def tokenize_normalize(s):\n",
    "    return [ word_normalizer(w) for w in s.split(' ') if len(w)>1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226833/226833 [00:03<00:00, 66465.68it/s]\n",
      " 11%|█         | 24495/226833 [00:24<03:21, 1002.64it/s]"
     ]
    }
   ],
   "source": [
    "data['ctext'] = data['ttext'].progress_apply(string_normalizer).progress_apply( tokenize_normalize )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcb0 =  get_vocabulary( data['ctext'] )\n",
    "print('словарь %i слов'%(len(vcb0)))\n",
    "# pd.DataFrame( vcb ).to_csv('voc0.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ctext'] = data['ctext'].apply( ' '.join  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext'].apply(lambda t:[ w.strip() for w in t.split() if w.strip() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t:[ re.sub(r'^http.*',' url ', w.strip() ) for w in t  ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t:[ re.sub(r'[:;]-*[)D]',' happysmile ', w.strip() )for w in t ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t:[ re.sub(r'\\)\\)\\)*',' happysmile ', w.strip() ) for w in t ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t:[ re.sub(r'[:;]\\*',' kisssmile ', w.strip() ) for w in t ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t:[ re.sub(r':\\(',' sadsmile ', w.strip() ) for w in t ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t:[ re.sub(r'\\(\\(\\(*',' sadsmile ', w.strip() ) for w in t ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = [ ' '.join(s) for s in data['ttext_clean'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].str.lower()\n",
    "data['ttext_clean'] = data['ttext_clean'].apply(lambda s: re.sub( r'\\W', ' ', s))\n",
    "data['ttext_clean'] = data['ttext_clean'].apply(lambda s: re.sub( r'_', ' ', s))\n",
    "data['ttext_clean'] = data['ttext_clean'].apply(lambda s: re.sub( r'\\b\\d+\\b', ' digit ', s)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(lambda t:[ w.strip() for w in t.split() if w.strip() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# замена буквенно-цифровых кодов\n",
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t: [w for w in t if not re.match( r'\\b.*\\d+.*\\b', w) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[['ttext_clean']]\n",
    "# data[['ttext']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import download as nltk_download\n",
    "# nltk_download('stopwords')\n",
    "\n",
    "# from Stemmer import Stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('russian') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gzip.open('../data/text/stop-nltk.txt.gz','rt',encoding='utf-8') as f: \n",
    "#     stopwords = set([ w.strip() for w in  f.read().split() if w.strip() ] )\n",
    "\n",
    "ppr(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление лишних слов\n",
    "data['ttext_clean'] = data['ttext_clean'].apply(lambda t:[w for w in t if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# from Stemmer import Stemmer\n",
    "# # pacman -S python-pystemmer\n",
    "# # pip install pystemmer\n",
    "\n",
    "# # стемминг, выделение основы слова\n",
    "# data['ttext_clean'] = data['ttext_clean'].apply( lambda t:Stemmer('russian').stemWords(t) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление коротких слов\n",
    "data['ttext_clean'] = data['ttext_clean'].apply(lambda t:[w for w in t if len(w)>2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[ data['ttext_clean'].str.len()<1 ][['ttext_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppr(data)\n",
    "data = data[ data['ttext_clean'].str.len()>0 ].reset_index(drop=True) \n",
    "ppr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## строим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<PAD>','<START>','<UNK>'] + sorted(set([ w for t in data['ttext_clean'] for w in t if w ]))\n",
    "ppr(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# w2v = Word2Vec( common_texts, min_count=1, size=256, window=4, workers=4)\n",
    "\n",
    "# # with open('result/Word2Vec.pkl', 'wb') as f: pickle.dump(w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { w:n for n,w in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply( lambda d: d+['<START>'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = data['ttext_clean'].str.len().max()\n",
    "n_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = ['<PAD>']*n_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['ttext_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_clean'] = data['ttext_clean'].apply(\n",
    "    lambda t: pad[len(t):] + list(reversed(t)) \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['ttext_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_code'] = data['ttext_clean'].apply(lambda t: [ vocab[w] for w in t ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ttext_code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppr(data)\n",
    "data = data.sample(32*7088).reset_index(drop=True)\n",
    "ppr(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack( data['ttext_code'].values).astype(np.float32 ) # , axis=-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y = data['ttype'].values\n",
    "y = OneHotEncoder(categories='auto').fit_transform(y.reshape(-1,1) ).todense().astype(np.float32)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X.npy',X)\n",
    "# np.save('y.npy',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# X = np.load('X.npy')\n",
    "# y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = int(X.max())\n",
    "X.shape , y.shape, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## строим нейросеть "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=226826\n",
    "# for i in range(1,n//2):\n",
    "#     if n%i==0: print(i)\n",
    "# # 23\n",
    "# # 46\n",
    "# # 4931\n",
    "# # 9862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=X.shape[1]\n",
    "batch_size=32\n",
    "num_classes=y.shape[1]\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "       input_dim=vocab_size, # e.g, 10 if you have 10 words in your vocabulary\n",
    "       output_dim=embedding_size, # size of the embedded vectors\n",
    "       input_length=time_steps,\n",
    "       batch_input_shape=(batch_size,time_steps)\n",
    "    ))\n",
    "\n",
    "model.add(LSTM(\n",
    "       32, \n",
    "       return_sequences=False, \n",
    "       stateful=False)\n",
    "    )\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X,y, batch_size=batch_size, epochs=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
